<div align="center">

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/01-ai/Yi/main/assets/img/Yi_logo_icon_dark.svg" width="200px">
  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/01-ai/Yi/main/assets/img/Yi_logo_icon_light.svg" width="200px"> 
  <img alt="specify theme context for images" src="https://raw.githubusercontent.com/01-ai/Yi/main/assets/img/Yi_logo_icon_light.svg" width="200px">
</picture>

</br>
</br>

<a href="https://github.com/01-ai/Yi/actions/workflows/build_docker_image.yml">
  <img src="https://github.com/01-ai/Yi/actions/workflows/build_docker_image.yml/badge.svg">
</a>
<a href="https://github.com/01-ai/Yi/blob/main/LICENSE">
  <img src="https://img.shields.io/badge/Code_License-Apache_2.0-lightblue">
</a>
<a href="https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt">
  <img src="https://img.shields.io/badge/Model_License-Yi_License-lightblue">
</a>
<a href="mailto:oss@01.ai">
  <img src="https://img.shields.io/badge/âœ‰ï¸-yi@01.ai-FFE01B">
</a>

</div>

<div align="center">
  <h3 align="center">Building the Next Generation of Open-Source and Bilingual LLMs</h3>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/01-ai" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://www.modelscope.cn/organization/01ai/" target="_blank">ModelScope</a> â€¢ âœ¡ï¸ <a href="https://wisemodel.cn/organization/01.AI" target="_blank">WiseModel</a>
</p> 

<p align="center">
    ğŸ‘©â€ğŸš€ Ask questions or discuss ideas on <a href="https://github.com/01-ai/Yi/discussions" target="_blank"> GitHub </a>!
</p> 

<p align="center">
    ğŸ‘‹ Join us on ğŸ’¬ <a href="https://github.com/01-ai/Yi/issues/43#issuecomment-1827285245" target="_blank"> WeChat (Chinese) </a>!
</p> 

<p align="center">
    ğŸ“š Grow at <a href="#learning-hub">Yi Learning Hub</a>!
</p>
<hr>

<ul>
  <li>ğŸ™Œ æœ¬æ–‡ç”± Yi å’Œå¿—æ„¿è€…å…±åŒç¿»è¯‘å®Œæˆï¼Œæ„Ÿè°¢æ¯ä¸€ä½ä¼ é€’çŸ¥è¯†çš„ç«ç‚¬æ‰‹ã€‚</li>

  <li>ğŸ¤— æ¬¢è¿å¤§å®¶ <a href="https://github.com/01-ai/Yi/discussions/314">åŠ å…¥æˆ‘ä»¬</a>ï¼Œå¼€å¯çŸ¥è¯†ä¹‹ç«æ—…ç¨‹ï¼Œå…±ç»˜æŠ€æœ¯å†…å®¹å›¾è°±ã€‚</li>
  
  <li>ğŸ“ æœ¬æ–‡ç¿»è¯‘ä½¿ç”¨äº† <a href="https://huggingface.co/spaces/01-ai/Yi-34B-Chat">Yi-34B-Chat</a>ï¼Œå…³äºç¿»è¯‘æ—¶ä½¿ç”¨çš„ prompt åŠæœ€ä½³å®è·µï¼Œå‚é˜… <a href="https://github.com/01-ai/Yi/wiki/%E7%BF%BB%E8%AF%91%E4%B8%8E%E5%AE%A1%E6%A0%A1%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF">ã€Œç¿»è¯‘ä¸å®¡æ ¡çš„æ­£ç¡®å§¿åŠ¿ã€</a>ã€‚</li>
</ul>



<!-- DO NOT REMOVE ME -->

<hr>

<details open>
<summary></b>ğŸ“• ç›®å½•</b></summary>


- [ğŸŸ¢ Yi æ˜¯ä»€ä¹ˆ?](#-yiæ˜¯ä»€ä¹ˆ)
  - [ğŸ“Œ ä»‹ç»](#-ä»‹ç»)
  - [ğŸ¯ æ¨¡å‹](#-models)
    - [èŠå¤©æ¨¡å‹](#èŠå¤©æ¨¡å‹)
    - [åŸºåº§æ¨¡å‹](#åŸºåº§æ¨¡å‹)
    - [å…¶ä»–ä¿¡æ¯](#å…¶ä»–ä¿¡æ¯)
  - [ğŸ‰ æœ€æ–°åŠ¨æ€](#-æœ€æ–°åŠ¨æ€)
- [ğŸŸ¢ How to use Yi?](#-how-to-use-yi)
  - [å¿«é€Ÿä¸Šæ‰‹](#quick-start)
    - [é€‰æ‹©ä½ çš„å­¦ä¹ è·¯å¾„](#choose-your-path)
    - [å¿«é€Ÿä¸Šæ‰‹ - ä½¿ç”¨ PyPiï¼ˆpip installï¼‰](#pip)
    - [å¿«é€Ÿä¸Šæ‰‹ - ä½¿ç”¨ llama.cpp é‡åŒ–è¿è¡Œ](#llamacpp-å¿«é€Ÿå…¥é—¨)
    - [å¿«é€Ÿä¸Šæ‰‹ - ä½¿ç”¨ Web Demo](#ç½‘é¡µç‰ˆæ¼”ç¤ºåº”ç”¨web-demo)
    - [å¿«é€Ÿä¸Šæ‰‹ - ä½¿ç”¨ Docker](#å¿«é€Ÿä¸Šæ‰‹---lm-studio)
    - [å¿«é€Ÿä¸Šæ‰‹ - ä½¿ç”¨ LM Studio](#LM-Studio)
  - [Fine tune](#fine-tune)
  - [Quantization](#quantization)
  - [éƒ¨ç½²](#éƒ¨ç½²)
  - [å­¦ä¹ ä¸­å¿ƒ](#å­¦ä¹ ä¸­å¿ƒ)
- [ğŸŸ¢ ä¸ºä»€ä¹ˆé€‰æ‹©Yiï¼Ÿ](#-ä¸ºä»€ä¹ˆé€‰æ‹©Yi?)
  - [ğŸŒ ç”Ÿæ€ç³»ç»Ÿ](#-ç”Ÿæ€ç³»ç»Ÿ)
    - [ğŸ’¦ ä¸Šæ¸¸](#-ä¸Šæ¸¸)
    - [ğŸŒŠ ä¸‹æ¸¸](#-ä¸‹æ¸¸)
      - [ğŸ”— æœåŠ¡](#-æœåŠ¡)
      - [âš™ï¸ é‡åŒ–](#ï¸-é‡åŒ–)
      - [ğŸ› ï¸ å¾®è°ƒ](#ï¸-å¾®è°ƒ)
      - [API](#api)
  - [ğŸ“Œ Benchmarks](#-benchmarks)
    - [ğŸ“Š Base model performance](#-base-model-performance)
    - [ğŸ“Š Chat model performance](#-chat-model-performance)
- [ğŸŸ¢ Who can use Yi?](#-who-can-use-yi)
- [ğŸŸ¢ Misc.](#-misc)
  - [Ackknowledgements](#acknowledgments)
  - [ğŸ“¡ Disclaimer](#-disclaimer)
  - [ğŸªª License](#-license)

</details>

<hr>

# ğŸŸ¢ Yi æ˜¯ä»€ä¹ˆ?

## ğŸ“Œ ä»‹ç»

- ğŸ¤– Yi ç³»åˆ—æ¨¡å‹æ˜¯ [01.AI](https://01.ai/) ä»é›¶è®­ç»ƒçš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚

- ğŸ™Œ Yi ç³»åˆ—æ¨¡å‹æ˜¯ä¸€ä¸ªåŒè¯­è¯­è¨€æ¨¡å‹ï¼Œåœ¨ 3T å¤šè¯­è¨€è¯­æ–™åº“ä¸Šè®­ç»ƒè€Œæˆï¼Œæ˜¯å…¨çƒæœ€å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚æœ¬ç³»åˆ—æ¨¡å‹åœ¨è¯­è¨€è®¤çŸ¥ã€å¸¸è¯†æ¨ç†ã€é˜…è¯»ç†è§£ç­‰æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ä¾‹å¦‚ï¼Œ

  - è‹±è¯­è¯­è¨€èƒ½åŠ›æ–¹é¢ï¼ŒYiç³»åˆ—æ¨¡å‹åœ¨2023å¹´12æœˆçš„ [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/)æ’è¡Œæ¦œä¸Šæ’åç¬¬äºŒï¼ˆä»…æ¬¡äºGPT-4ï¼‰ï¼Œè¶…è¿‡äº†å…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚ LLaMA2-chat-70Bã€Claude 2 å’Œ ChatGPTã€‚

  - ä¸­æ–‡è¯­è¨€èƒ½åŠ›æ–¹é¢ï¼ŒYiç³»åˆ—æ¨¡å‹åœ¨2023å¹´10æœˆçš„ [SuperCLUE](https://www.superclueai.com/)æ’è¡Œæ¦œä¸Šæ’åç¬¬äºŒï¼ˆä»…æ¬¡äºGPT-4ï¼‰ï¼Œè¶…è¿‡äº†å…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ç™¾åº¦ERNIEã€Qwen å’Œ Baichuanã€‚
  - ğŸ™ ï¼ˆæ„Ÿè°¢ LLaMA ï¼‰æ„Ÿè°¢ Transformer å’Œ LLaMA å¼€æºç¤¾åŒºï¼Œç®€åŒ–äº† [01.AI](https://01.ai/) ä»é›¶å¼€å§‹æ„å»ºå¤§æ¨¡å‹çš„å·¥ä½œï¼Œ[01.AI](https://01.ai/) ä¹Ÿèƒ½å¤Ÿåœ¨äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿä¸­ä½¿ç”¨ç›¸åŒçš„å·¥å…·ã€‚

  <details style="display: inline;"><summary> å¦‚æœä½ å¯¹ Yi é‡‡ç”¨LLaMAæ¶æ„åŠå…¶è®¸å¯ä½¿ç”¨æ”¿ç­–æ„Ÿå…´è¶£ï¼Œè¯·å‚é˜… <span style="color:  green;"> Yi ä¸ LLaMA çš„å…³ç³»</span> â¬‡ï¸</summary> <ul> <br>

> ğŸ’¡ ç®€çŸ­æ€»ç»“
> 
> Yiç³»åˆ—æ¨¡å‹é‡‡ç”¨æ¨¡å‹æ¶æ„ä¸LLaMAç›¸åŒï¼Œä½†å®ƒä»¬**ä¸æ˜¯**LLaMAçš„è¡ç”Ÿå“ã€‚


- Yiå’ŒLLaMAéƒ½æ˜¯åŸºäºTransformerç»“æ„æ„å»ºçš„ã€‚å®é™…ä¸Šï¼Œè‡ª2018å¹´ä»¥æ¥ï¼ŒTransformerä¸€ç›´æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å¸¸ç”¨æ¶æ„ã€‚

- åœ¨Transformeræ¶æ„çš„åŸºç¡€ä¸Šï¼ŒLLaMAå‡­å€Ÿå‡ºè‰²çš„ç¨³å®šæ€§ã€å¯é çš„æ”¶æ•›æ€§å’Œå¼ºå¤§çš„å…¼å®¹æ€§ï¼Œæˆä¸ºå¤§å¤šæ•°å…ˆè¿›å¼€æºæ¨¡å‹çš„åŸºçŸ³ã€‚å› æ­¤ï¼ŒLLaMAä¹Ÿæˆä¸ºYiç­‰æ¨¡å‹çš„åŸºç¡€æ¡†æ¶ã€‚

- å¾—ç›ŠäºTransformerå’ŒLLaMAæ¶æ„ï¼Œå…¶ä»–æ¨¡å‹å¯ä»¥ç®€åŒ–ä»é›¶å¼€å§‹æ„å»ºæ¨¡å‹çš„å·¥ä½œï¼Œå¹¶èƒ½å¤Ÿåœ¨å„è‡ªçš„ç”Ÿæ€ç³»ç»Ÿä¸­ä½¿ç”¨ç›¸åŒçš„å·¥å…·ã€‚

- ç„¶è€Œï¼ŒYiç³»åˆ—æ¨¡å‹ä¸æ˜¯LLaMAçš„è¡ç”Ÿå“ï¼Œå› ä¸ºå®ƒä»¬ä¸ä½¿ç”¨LLaMAçš„æƒé‡ã€‚

  - è™½ç„¶å¤§å¤šæ•°å¼€æºæ¨¡å‹éƒ½é‡‡ç”¨äº†LLaMAçš„ç»“æ„ï¼Œä½†å†³å®šæ¨¡å‹è¡¨ç°çš„å…³é”®å› ç´ æ˜¯è®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ã€æµæ°´çº¿åŠå…¶åŸºç¡€è®¾æ–½ã€‚

  - [01.AI](https://01.ai/) ç”¨ç‹¬ç‰¹çš„æ–¹å¼å¼€å‘äº†Yiï¼Œä»é›¶å¼€å§‹ç‹¬ç«‹åˆ›å»ºäº†è‡ªå·±çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®é›†ã€é«˜æ•ˆçš„è®­ç»ƒæµæ°´çº¿å’Œå¼ºå¤§çš„è®­ç»ƒåŸºç¡€è®¾æ–½ï¼Œå› æ­¤Yiç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†å“è¶Šçš„æˆç»©ï¼Œåœ¨2023å¹´12æœˆçš„[Alpaca Leaderboard](https://tatsu-lab.github.io/alpaca_eval/)ä¸Šæ’åä»…æ¬¡äºGPT4ï¼Œè¶…è¿‡äº†LLaMAã€‚
</ul>
</details>

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">å›åˆ°é¡¶éƒ¨ â¬†ï¸ </a> ] </div>

## ğŸ‰ æœ€æ–°åŠ¨æ€

<details open>
  <summary>ğŸ¯ <b>2024å¹´01æœˆ23æ—¥</b>: <code><a href="https://huggingface.co/01-ai/Yi-VL-34B">Yi-VL-34B</a></code> å’Œ <code><a href="https://huggingface.co/01-ai/Yi-VL-6B">Yi-VL-6B</a></code>çš„å¤šæ¨¡æ€è¯­è¨€å¤§æ¨¡å‹ï¼Œå‡å·²å¼€æºå¹¶å¯¹å…¬ä¼—å¼€æ”¾ã€‚</summary>
  <br>
   åœ¨<a href="https://arxiv.org/abs/2311.16502">MMMU</a> å’Œ <a href="https://arxiv.org/abs/2401.11944">CMMMU</a>æœ€æ–°çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ˆæˆªè‡³2024å¹´1æœˆçš„å¯ç”¨æ•°æ®ï¼‰ï¼Œ<code><a href="https://huggingface.co/01-ai/Yi-VL-34B">Yi-VL-34B</a></code>è£ç™»æ¦œé¦–ã€‚</li>
</details>

<details>
<summary>ğŸ¯ <b>2023/11/23</b>: å…­å¤§èŠå¤©æ¨¡å‹å‡å·²å¼€æºå¹¶å¯¹å…¬ä¼—å¼€æ”¾ã€‚</summary>
<br>
å‘å¸ƒäº†ä¸¤ä¸ªèŠå¤©æ¨¡å‹ï¼Œéƒ½æ˜¯åŸºäºä¹‹å‰å‘å¸ƒçš„ä¸¤ä¸ªåŸºåº§æ¨¡å‹ï¼›ä¹Ÿå‘å¸ƒäº†ç”± GPTQ é‡åŒ–çš„ä¸¤ä¸ª8ä½æ¨¡å‹å’Œç”± AWQ é‡åŒ–çš„ä¸¤ä¸ª4ä½æ¨¡å‹ã€‚

- `Yi-34B-Chat`
- `Yi-34B-Chat-4bits`
- `Yi-34B-Chat-8bits`
- `Yi-6B-Chat`
- `Yi-6B-Chat-4bits`
- `Yi-6B-Chat-8bits`

ä½ å¯ä»¥è®¿é—®ä»¥ä¸‹é“¾æ¥è¿›è¡Œè¯•ç”¨ï¼š

- [Hugging Face](https://huggingface.co/spaces/01-ai/Yi-34B-Chat)
- [Replicate](https://replicate.com/01-ai)
</details>

<details>
<summary>ğŸ”” <b>2023/11/23</b>: Yiç³»åˆ—æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®æ›´æ–°è‡³ v2.1 ç‰ˆæœ¬ã€‚</summary>
</details>

<details> 
<summary>ğŸ”¥ <b>2023/11/08</b>: Yi-34B èŠå¤©æ¨¡å‹å¼€å§‹é‚€è¯·æµ‹è¯•ã€‚</summary>
<br>
å‚ä¸æµ‹è¯•ç”³è¯·è¡¨ï¼š

- [è‹±æ–‡](https://cn.mikecrm.com/l91ODJf)
- [ä¸­æ–‡](https://cn.mikecrm.com/gnEZjiQ)

</details>

<details>
<summary>ğŸ¯ <b>2023/11/05</b>: <code>Yi-6B-200K</code> å’Œ <code>Yi-34B-200K</code> çš„åŸºåº§æ¨¡å‹å‡å·²å¼€æºå¹¶å¯¹å…¬ä¼—å¼€æ”¾ã€‚ </summary>
<br>
å‘å¸ƒäº†ä¸¤ä¸ªä¸ä¹‹å‰å‘å¸ƒå‚æ•°è§„æ¨¡ç›¸åŒçš„åŸºåº§æ¨¡å‹ï¼Œåªæ˜¯ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ°äº†200Kã€‚

</details>

<details>
<summary>ğŸ¯ <b>2023/11/02</b>: <code>Yi-6B</code> å’Œ <code>Yi-34B</code> çš„åŸºåº§æ¨¡å‹å‡å·²å¼€æºå¹¶å¯¹å…¬ä¼—å¼€æ”¾ã€‚</summary>
<br>
é¦–æ¬¡å…¬å¼€å‘å¸ƒäº†ä¸¤ä¸ªåŒè¯­ï¼ˆè‹±è¯­/ä¸­æ–‡ï¼‰åŸºåº§æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡åˆ†åˆ«ä¸º6Bå’Œ34Bã€‚ä¸¤è€…å‡ä»¥4Kåºåˆ—é•¿åº¦è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶å¯æ‰©å±•åˆ°32Kã€‚

</details>

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">å›åˆ°é¡¶éƒ¨ â¬†ï¸ </a> ] </div>

## ğŸ¯ æ¨¡å‹
Yiæ¨¡å‹æœ‰å¤šç§å‚æ•°è§„æ¨¡ï¼Œé€‚ç”¨äºä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚ä½ ä¹Ÿå¯ä»¥å¯¹Yiæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæ»¡è¶³ç‰¹å®šéœ€æ±‚ã€‚

å¦‚æœä½ æƒ³è¦éƒ¨ç½²Yiæ¨¡å‹ï¼Œè¯·ç¡®ä¿æ‚¨çš„è½¯ä»¶å’Œç¡¬ä»¶æ»¡è¶³[éƒ¨ç½²è¦æ±‚](#deployment).

### èŠå¤©æ¨¡å‹

| æ¨¡å‹ | ä¸‹è½½ 
|---|---
Yi-34B-Chat	| â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-34B-Chat)  â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-34B-Chat/summary)
Yi-34B-Chat-4bits	| â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-34B-Chat-4bits)  â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-34B-Chat-4bits/summary)
Yi-34B-Chat-8bits | â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-34B-Chat-8bits) â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-34B-Chat-8bits/summary)
Yi-6B-Chat| â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-6B-Chat) â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-6B-Chat/summary)
Yi-6B-Chat-4bits |	â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-6B-Chat-4bits)  â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-6B-Chat-4bits/summary)
Yi-6B-Chat-8bits	|  â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-6B-Chat-8bits) â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-6B-Chat-8bits/summary)

<sub><sup> - 4-bitç³»åˆ—æ¨¡å‹ç”±AWQé‡åŒ–ã€‚<br> - 8-bitç³»åˆ—æ¨¡å‹ç”±GPTQé‡åŒ–ã€‚<br> - æ‰€æœ‰é‡åŒ–æ¨¡å‹éƒ½å…·æœ‰è¾ƒä½çš„ä½¿ç”¨é—¨æ§›ï¼Œå› æ­¤å®ƒä»¬å¯ä»¥åœ¨æ¶ˆè´¹çº§GPUï¼ˆä¾‹å¦‚3090ã€4090ï¼‰ä¸Šéƒ¨ç½²ã€‚</sup></sub>
### åŸºåº§æ¨¡å‹

| æ¨¡å‹ | ä¸‹è½½ | 
|---|---|
Yi-34B| â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-34B)  â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-34B/summary)
Yi-34B-200K|â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-34B-200K)  â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-34B-200K/summary)
Yi-6B| â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-6B)  â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-6B/summary)
Yi-6B-200K	| â€¢ [ğŸ¤— Hugging Face](https://huggingface.co/01-ai/Yi-6B-200K) â€¢ [ğŸ¤– ModelScope](https://www.modelscope.cn/models/01ai/Yi-6B-200K/summary)

<sub><sup> - 200k å¤§çº¦ç›¸å½“äº 40 ä¸‡ä¸ªæ±‰å­—ã€‚</sup></sub>

### å…¶ä»–ä¿¡æ¯

- èŠå¤©å’ŒåŸºåº§æ¨¡å‹ï¼š

  - 6B ç³»åˆ—çš„æ¨¡å‹é€‚åˆä¸ªäººå’Œå­¦æœ¯ä½¿ç”¨ã€‚

  - 34B ç³»åˆ—çš„æ¨¡å‹é€‚åˆä¸ªäººã€å­¦æœ¯å’Œå•†ä¸šç”¨é€”ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºä¸­å°å‹ä¼ä¸šï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæ€§ä»·æ¯”é«˜çš„è§£å†³æ–¹æ¡ˆï¼Œä»·æ ¼åˆç†ï¼Œèƒ½åŠ›è¶…å‡ºé¢„æœŸã€‚

  - **é»˜è®¤çš„ä¸Šä¸‹æ–‡çª—å£**æ˜¯ **4k tokens**ã€‚

  - é¢„è®­ç»ƒçš„ tokens æ•°é‡æ˜¯ 3Tã€‚

  - è®­ç»ƒæ•°æ®æˆªè‡³ 2023 å¹´ 6 æœˆã€‚

- èŠå¤©æ¨¡å‹
  
  <details style="display: inline;"><summary>å…³äºèŠå¤©æ¨¡å‹çš„å±€é™æ€§ï¼Œè§ä»¥ä¸‹è§£é‡Šã€‚ â¬‡ï¸</summary> 
   <ul>
   <br> <a href="https://01.ai/">01.AI</a> å‘å¸ƒçš„èŠå¤©æ¨¡å‹åœ¨ç‹¬å®¶è®­ç»ƒä¸­é‡‡ç”¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŠ€æœ¯ã€‚ä¸å…¶ä»–æ ‡å‡†èŠå¤©æ¨¡å‹ç›¸æ¯”ï¼Œ <a href="https://01.ai/">01.AI</a> çš„æ¨¡å‹ç”Ÿæˆçš„å›å¤æ›´åŠ å¤šæ ·åŒ–ï¼Œå› æ­¤é€‚ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ¯”å¦‚åˆ›æ„åœºæ™¯ã€‚æ­¤å¤–ï¼Œå›å¤æ›´åŠ å¤šæ ·åŒ–ï¼Œæœ‰åˆ©äºæé«˜å›å¤çš„è´¨é‡ï¼Œå¯¹åç»­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¸®åŠ©å¾ˆå¤§ã€‚

    <br>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå›å¤å¤šæ ·åŒ–ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´æŸäº›å·²çŸ¥é—®é¢˜æ›´åŠ ä¸¥é‡ï¼ŒåŒ…æ‹¬ï¼š
      <li>è™šæ„ï¼šå³æ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆäº‹å®é”™è¯¯æˆ–ä¸è¿è´¯çš„ä¿¡æ¯ã€‚æ¨¡å‹å›å¤å¤šæ ·åŒ–ï¼Œæ›´æœ‰å¯èƒ½å‡ºç°è™šæ„çš„ç°è±¡ï¼Œè¿™äº›è™šæ„çš„å›å¤å¯èƒ½ä¸æ˜¯åŸºäºå‡†ç¡®çš„æ•°æ®æˆ–é€»è¾‘æ¨ç†ã€‚</li>
      <li>é‡æ–°ç”Ÿæˆçš„å›å¤ä¸ä¸€è‡´ï¼šé‡æ–°ç”Ÿæˆå›å¤æˆ–è€…å¯¹å›å¤è¿›è¡Œé‡‡æ ·æ—¶ï¼Œç»“æœå¯èƒ½å‡ºç°å‰åä¸ä¸€è‡´ã€‚å¤šæ ·æ€§å¢å¤šä¼šå¯¼è‡´å³ä½¿åœ¨ç›¸ä¼¼çš„è¾“å…¥æ¡ä»¶ä¸‹ï¼Œç»“æœä¹Ÿä¼šå­˜åœ¨å·®å¼‚ã€‚</li>
      <li>ç´¯ç§¯è¯¯å·®ï¼šå½“æ¨¡å‹å›å¤çš„é”™è¯¯éšæ—¶é—´ç´¯ç§¯ï¼Œå°±ä¼šå‡ºç°ç´¯è®¡è¯¯å·®çš„ç°è±¡ã€‚æ¨¡å‹å›å¤çš„å¤šæ ·åŒ–å¢åŠ äº†å°è¯¯å·®ç§¯ç´¯æˆå¤§é”™è¯¯çš„å¯èƒ½æ€§ï¼Œè¿™ç§æƒ…å†µå¸¸è§äºæ‰©å±•æ¨ç†ã€è§£å†³æ•°å­¦é—®é¢˜ç­‰å¤æ‚ä»»åŠ¡ä¸­ç­‰ã€‚</li>
      <li>ä¸ºäº†è·å¾—æ›´è¿è´¯ä¸€è‡´çš„å›ç­”ï¼Œå»ºè®®è°ƒæ•´ç”Ÿæˆé…ç½®å‚æ•°ï¼Œå¦‚æ¸©åº¦ã€top_p æˆ– top_kã€‚è¿™äº›è°ƒæ•´æœ‰åˆ©äºå¹³è¡¡æ¨¡å‹å›å¤çš„åˆ›é€ æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ul>
</details>

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">å›åˆ°é¡¶éƒ¨ â¬†ï¸ </a> ] </div>


# ğŸŸ¢ How to use Yi?

- [Quick start](#quick-start)
  - [Choose your path](#choose-your-parth)
  - [pip](#pip)
  - [llama.cpp](https://github.com/01-ai/Yi/blob/main/docs/yi_llama.cpp.md)
  - [å¿«é€Ÿä¸Šæ‰‹ - ä½¿ç”¨ LM Studio](#å¿«é€Ÿä¸Šæ‰‹---lm-studio)
  - [Web demo](#web-demo)
- [Fine tune](#fine-tune)
- [Quantization](#quantization)
- [Deployment](https://github.com/01-ai/Yi/blob/main/docs/deployment.md)
- [Learning hub](https://github.com/01-ai/Yi/blob/main/docs/learning_hub.md)

## Quick start

Getting up and running with Yi models is simple with multiple choices available. 

### Choose your path

Select one of the following paths to begin your journey with Yi!

 ![Quick start - Choose your path](https://github.com/01-ai/Yi/blob/main/assets/img/quick_start_path.png)

#### ğŸ¯ Deploy Yi locally

If you prefer to deploy Yi models locally, 

  - ğŸ™‹â€â™€ï¸ and you have **sufficient** resources (for example, NVIDIA A800 80GB), you can choose one of the following methods:
    - [pip](#pip)
    - [Docker](https://github.com/01-ai/Yi/blob/main/docs/README_legacy_cn.md#11-docker)
    - [conda-lock](https://github.com/01-ai/Yi/blob/main/docs/README_legacy.md#12-local-development-environment)

  - ğŸ™‹â€â™€ï¸ and you have **limited** resources (for example, a MacBook Pro), you can use [llama.cpp](#quick-start---llamacpp)

#### ğŸ¯ Not to deploy Yi locally

If you prefer not to deploy Yi models locally, you can explore Yi's capabilities using any of the following options.

##### ğŸ™‹â€â™€ï¸ Run Yi with APIs

If you want to explore more features of Yi, you can adopt one of these methods:

- Yi APIs (Yi official)
  - [Early access has been granted](https://x.com/01AI_Yi/status/1735728934560600536?s=20) to some applicants. Stay tuned for the next round of access!

- [Yi APIs](https://replicate.com/01-ai/yi-34b-chat/api?tab=nodejs) (Replicate)

##### ğŸ™‹â€â™€ï¸ Run Yi in playground

If you want to chat with Yi with more customizable options (e.g., system prompt, temperature, repetition penalty, etc.), you can try one of the following options:
  
  - [Yi-34B-Chat-Playground](https://platform.lingyiwanwu.com/prompt/playground) (Yi official)
    - Access is available through a whitelist. Welcome to apply (fill out a form in [English](https://cn.mikecrm.com/l91ODJf) or [Chinese](https://cn.mikecrm.com/gnEZjiQ)).
  
  - [Yi-34B-Chat-Playground](https://replicate.com/01-ai/yi-34b-chat) (Replicate) 

##### ğŸ™‹â€â™€ï¸ Chat with Yi

 If you want to chat with Yi, you can use one of these online services, which offer a similar user experience:

- [Yi-34B-Chat](https://huggingface.co/spaces/01-ai/Yi-34B-Chat) (Yi official on Hugging Face)
  - No registration is required.

- [Yi-34B-Chat](https://platform.lingyiwanwu.com/) (Yi official beta)
  - Access is available through a whitelist. Welcome to apply (fill out a form in [English](https://cn.mikecrm.com/l91ODJf) or [Chinese](https://cn.mikecrm.com/gnEZjiQ)).

### Quick start - pip

This tutorial guides you through every step of running **Yi-34B-Chat locally on an A800 (80G)** and then performing inference.

#### Step 0: Prerequistes
 
- Make sure Python 3.10 or a later version is installed.

- If you want to run other Yi models, see [software and hardware requirements](#deployment)

#### Step 1: Prepare your environment 

To set up the environment and install the required packages, execute the following command.

```bash
git clone https://github.com/01-ai/Yi.git
cd yi
pip install -r requirements.txt
```

#### Step 2: Download the Yi model

You can download the weights and tokenizer of Yi models from the following sources:

- [Hugging Face](https://huggingface.co/01-ai)
- [ModelScope](https://www.modelscope.cn/organization/01ai/)
- [WiseModel](https://wisemodel.cn/organization/01.AI)

#### Step 3: Perform inference

You can perform inference with Yi chat or base models as below.

##### Perform inference with Yi chat model

1. Create a file named  `quick_start.py` and copy the following content to it.

    ```python
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model_path = '<your-model-path>'

    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)

    # Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype='auto'
    ).eval()

    # Prompt content: "hi"
    messages = [
        {"role": "user", "content": "hi"}
    ]

    input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
    output_ids = model.generate(input_ids.to('cuda'))
    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

    # Model response: "Hello! How can I assist you today?"
    print(response)
    ```

2. Run `quick_start.py`.

    ```bash
    python quick_start.py
    ```

    Then you can see an output similar to the one below. ğŸ¥³

    ```bash
    Hello! How can I assist you today?
    ```

##### Perform inference with Yi base model

The steps are similar to [pip - Perform inference with Yi chat model](#perform-inference-with-yi-chat-model).

You can use the existing file [`text_generation.py`](https://github.com/01-ai/Yi/tree/main/demo).

```bash
python demo/text_generation.py  --model <your-model-path>
```

Then you can see an output similar to the one below. ğŸ¥³

<details>

<summary>Output â¬‡ï¸ </summary>

<br>

**Prompt**: Let me tell you an interesting story about cat Tom and mouse Jerry,

**Generation**: Let me tell you an interesting story about cat Tom and mouse Jerry, which happened in my childhood. My father had a big house with two cats living inside it to kill mice. One day when I was playing at home alone, I found one of the tomcats lying on his back near our kitchen door, looking very much like he wanted something from us but couldnâ€™t get up because there were too many people around him! He kept trying for several minutes before finally giving up...

</details>

### å¿«é€Ÿä¸Šæ‰‹ - Docker

<details>
<summary> ğŸš€ ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨æœ¬åœ°è¿è¡Œ Yi-34B-Chat æ¨¡å‹çš„è¯¦ç»†æ•™ç¨‹ ğŸ‘‡</summary>
<br>è¿™ä¸ªæ•™ç¨‹å°†æŒ‡å¯¼ä½ å¦‚ä½•åœ¨æœ¬åœ° A800 GPU ä¸Šè¿è¡Œ <strong>Yi-34B-Chat</strong> æ¨¡å‹ï¼Œå¹¶æ‰§è¡Œæ¨ç†ã€‚
<h4>æ­¥éª¤0: å‡†å¤‡å·¥ä½œ</h4>
<p>ç¡®ä¿ä½ å·²ç»å®‰è£…äº† <a href="https://docs.docker.com/engine/install/?open_in_browser=true">Docker</a> å’Œ <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">nvidia-container-toolkit</a>ã€‚</p>
<h4>æ­¥éª¤1: å¯åŠ¨ Docker å®¹å™¨</h4>
<pre><code>docker run -it --gpus all \
-v &lt;your-model-path&gt;: /models
ghcr.io/01-ai/yi:latest
</code></pre>
<p>æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥ä»<code>registry.lingyiwanwu.com/ci/01-ai/yi:latest</code> æ‹‰å–å·²ç»æ„å»ºå¥½çš„ Yi Docker é•œåƒã€‚</p>

<h4>æ­¥éª¤2: æ‰§è¡Œæ¨ç†</h4>
    <p>ä½ å¯ä»¥ä½¿ç”¨Yi çš„èŠå¤©æ¨¡å‹æˆ–åŸºç¡€æ¨¡å‹æ¥æ‰§è¡Œæ¨ç†ã€‚</p>
    
<h5>ä½¿ç”¨ Yi èŠå¤©æ¨¡å‹æ‰§è¡Œæ¨ç†</h5>
    <p>æ‰§è¡Œæ¨ç†çš„æ­¥éª¤ä¸ä½¿ç”¨<a href="#perform-inference-with-yi-chat-model">pipå®‰è£…æŒ‡å—</a>ç±»ä¼¼ã€‚</p>
    <p><strong>è¯·æ³¨æ„</strong> å”¯ä¸€ä¸åŒçš„æ˜¯ä½ éœ€è¦è®¾ç½® <code>model_path = '&lt;your-model-mount-path&gt;'</code> è€Œä¸æ˜¯ <code>model_path = '&lt;your-model-path&gt;'</code>ã€‚</p>
<h5>ä½¿ç”¨ Yi åŸºç¡€æ¨¡å‹æ‰§è¡Œæ¨ç†</h5>
    <p>æ‰§è¡Œæ¨ç†çš„æ­¥éª¤ä¸ä½¿ç”¨<a href="#perform-inference-with-yi-chat-model">pipå®‰è£…æŒ‡å—</a>ç±»ä¼¼ã€‚</p>
    <p><strong>è¯·æ³¨æ„</strong> å”¯ä¸€ä¸åŒçš„æ˜¯ä½ éœ€è¦è®¾ç½® <code>--model &lt;your-model-mount-path&gt;'</code> è€Œä¸æ˜¯ <code>model &lt;your-model-path&gt;</code>ã€‚</p>
</details>



### å¿«é€Ÿä¸Šæ‰‹ - llama.cpp
<details>
<summary> ğŸš€ ä»¥ä¸‹æ˜¯ä½¿ç”¨ llama.cpp åœ¨æœ¬åœ°è¿è¡Œ Yi-chat-6B-2bits æ¨¡å‹çš„è¯¦ç»†æ•™ç¨‹ğŸ‘‡ </summary> 
<br>è¯¥æ•™ç¨‹åˆ†äº«å¦‚ä½•åœ¨æœ¬åœ°è¿è¡Œ <a href="https://huggingface.co/XeIaso/yi-chat-6B-GGUF/tree/main">Yi-chat-6B-2bits</a> é‡åŒ–æ¨¡å‹ï¼Œå¹¶ä¸”è¿›è¡Œæ¨ç†ã€‚</p>

- [æ­¥éª¤ 0: å‰ææ¡ä»¶](#step-0-prerequisites)
- [æ­¥éª¤ 1: ä¸‹è½½ llama.cpp](#step-1-download-llamacpp)
- [æ­¥éª¤ 2: ä¸‹è½½ Yi æ¨¡å‹](#step-2-download-yi-model)
- [æ­¥éª¤ 3: è¿›è¡Œæ¨ç†](#step-3-perform-inference)

#### æ­¥éª¤ 0: å‰ææ¡ä»¶

- è¯¥æ•™ç¨‹åœ¨ MacBook Proï¼ˆ16GB å†…å­˜å’Œ Apple M2 Pro èŠ¯ç‰‡ï¼‰ä¸Šè¿è¡Œ ã€‚

- ç¡®ä¿ä½ çš„ç”µè„‘ä¸Šå®‰è£…äº† [`git-lfs`](https://git-lfs.com/) ã€‚
  
#### æ­¥éª¤ 1: ä¸‹è½½ `llama.cpp`

å…‹éš† [`llama.cpp`](https://github.com/ggerganov/llama.cpp) ä»“åº“ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
git clone git@github.com:ggerganov/llama.cpp.git
```

#### æ­¥éª¤ 2: ä¸‹è½½ Yi æ¨¡å‹

æ­¥éª¤ 2.1ï¼šä»…ä¸‹è½½ [XeIaso/yi-chat-6B-GGUF](https://huggingface.co/XeIaso/yi-chat-6B-GGUF/tree/main) ä»“åº“çš„ pointersï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```bash
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/XeIaso/yi-chat-6B-GGUF
```

æ­¥éª¤ 2.2ï¼šä¸‹è½½é‡åŒ–åçš„ Yi æ¨¡å‹ [yi-chat-6b.Q2_K.gguf](https://huggingface.co/XeIaso/yi-chat-6B-GGUF/blob/main/yi-chat-6b.Q2_K.gguf)ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
git-lfs pull --include yi-chat-6b.Q2_K.gguf
```

#### æ­¥éª¤ 3: æ‰§è¡Œæ¨ç†

å¦‚éœ€ä½“éªŒ Yi æ¨¡å‹ï¼ˆè¿›è¡Œæ¨¡å‹æ¨ç†ï¼‰ï¼Œä½ å¯ä»¥é€‰æ‹©ä»¥ä¸‹ä»»æ„ä¸€ç§æ–¹æ³•ã€‚

- [æ–¹æ³• 1ï¼šåœ¨ç»ˆç«¯ä¸­æ‰§è¡Œæ¨ç†](#method-1-perform-inference-in-terminal)
  
- [æ–¹æ³• 2ï¼šåœ¨ç½‘é¡µä¸Šæ‰§è¡Œæ¨ç†](#method-2-perform-inference-in-web)

### æ–¹æ³•ä¸€ï¼šåœ¨ç»ˆç«¯ä¸­æ‰§è¡Œæ¨ç†

æœ¬æ–‡ä½¿ç”¨ 4 ä¸ªçº¿ç¨‹ç¼–è¯‘ `llama.cpp` ï¼Œä¹‹åè¿›è¡Œæ¨ç†ã€‚åœ¨ `llama.cpp` æ‰€åœ¨çš„ç›®å½•ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

> ### æç¤º
>
> - å°† `/Users/yu/yi-chat-6B-GGUF/yi-chat-6b.Q2_K.gguf` æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹çš„å®é™…è·¯å¾„ã€‚
>
> - é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ˜¯ç»­å†™æ¨¡å¼ã€‚
> - å¦‚éœ€æŸ¥çœ‹æ›´å¤šè‡ªå®šä¹‰é€‰é¡¹ï¼ˆä¾‹å¦‚ï¼Œç³»ç»Ÿæç¤ºã€æ¸©åº¦ã€é‡å¤æƒ©ç½šç­‰ï¼‰ï¼Œè¿è¡Œ `./main -h` æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨è¯´æ˜ã€‚

```bash
make -j4 && ./main -m /Users/yu/yi-chat-6B-GGUF/yi-chat-6b.Q2_K.gguf -p "How do you feed your pet fox? Please answer this question in 6 simple steps:\nStep 1:" -n 384 -e

...

How do you feed your pet fox? Please answer this question in 6 simple steps:

Step 1: Select the appropriate food for your pet fox. You should choose high-quality, balanced prey items that are suitable for their unique dietary needs. These could include live or frozen mice, rats, pigeons, or other small mammals, as well as fresh fruits and vegetables.

Step 2: Feed your pet fox once or twice a day, depending on the species and its individual preferences. Always ensure that they have access to fresh water throughout the day.

Step 3: Provide an appropriate environment for your pet fox. Ensure it has a comfortable place to rest, plenty of space to move around, and opportunities to play and exercise.

Step 4: Socialize your pet with other animals if possible. Interactions with other creatures can help them develop social skills and prevent boredom or stress.

Step 5: Regularly check for signs of illness or discomfort in your fox. Be prepared to provide veterinary care as needed, especially for common issues such as parasites, dental health problems, or infections.

Step 6: Educate yourself about the needs of your pet fox and be aware of any potential risks or concerns that could affect their well-being. Regularly consult with a veterinarian to ensure you are providing the best care.

...

```

æ­å–œä½ ï¼ä½ å·²ç»æˆåŠŸåœ°å‘ Yi æ¨¡å‹æå‡ºäº†é—®é¢˜å¹¶å¾—åˆ°äº†å›ç­”ï¼ğŸ¥³

### æ–¹æ³•äºŒï¼šåœ¨ç½‘é¡µä¸Šè¿›è¡Œæ¨ç†

1. åˆå§‹åŒ–ä¸€ä¸ªè½»é‡çº§ã€å¿«é€Ÿçš„èŠå¤©æœºå™¨äººï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

    ```bash
    ./server --ctx-size 2048 --host 0.0.0.0 --n-gpu-layers 64 --model /Users/yu/yi-chat-6B-GGUF/yi-chat-6b.Q2_K.gguf
    ```

    ä½ å°†ä¼šçœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

    ```bash
    ...

    llama_new_context_with_model: n_ctx      = 2048
    llama_new_context_with_model: freq_base  = 5000000.0
    llama_new_context_with_model: freq_scale = 1
    ggml_metal_init: allocating
    ggml_metal_init: found device: Apple M2 Pro
    ggml_metal_init: picking default device: Apple M2 Pro
    ggml_metal_init: ggml.metallib not found, loading from source
    ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil
    ggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'
    ggml_metal_init: GPU name:   Apple M2 Pro
    ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)
    ggml_metal_init: hasUnifiedMemory              = true
    ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
    ggml_metal_init: maxTransferRate               = built-in GPU
    ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)
    llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB
    ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)
    llama_build_graph: non-view tensors processed: 676/676
    llama_new_context_with_model: compute buffer total size = 159.19 MiB
    ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)
    Available slots:
    -> Slot 0 - max context: 2048

    llama server listening at http://0.0.0.0:8080
    ```

2. è®¿é—®èŠå¤©æœºå™¨äººç•Œé¢ï¼Œæ‰“å¼€ä½ çš„ç½‘ç»œæµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ `http://0.0.0.0:8080`ã€‚

    ![Yiæ¨¡å‹èŠå¤©æœºå™¨äººç•Œé¢ - llama.cpp](https://github.com/01-ai/Yi/blob/main/assets/img/yi_llama_cpp1.png)

3. åœ¨æç¤ºçª—å£ä¸­è¾“å…¥ä¸€ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚ï¼Œâ€œå¦‚ä½•å–‚å…»ä½ çš„å® ç‰©ç‹ç‹¸ï¼Ÿè¯·ç”¨ 6 ä¸ªç®€å•çš„æ­¥éª¤å›ç­”â€ï¼Œä½ å°†ä¼šæ”¶åˆ°ä¸€ä¸ªç­”æ¡ˆã€‚

    ![å‘ Yi æ¨¡å‹æé—® - llama.cpp](https://github.com/01-ai/Yi/blob/main/assets/img/yi_llama_cpp2.png)

</ul>
</details>

### å¿«é€Ÿä¸Šæ‰‹ - Web Demo

ä½ å¯ä»¥ä½¿ç”¨ Yi **èŠå¤©æ¨¡å‹**ï¼ˆYi-34B-Chatï¼‰æ‰“é€ ä¸€ä¸ª Web Demoã€‚æ³¨æ„ï¼šYi åŸºç¡€æ¨¡å‹ï¼ˆYi-34Bï¼‰ä¸æ”¯æŒè¯¥åŠŸèƒ½ã€‚

[ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ç¯å¢ƒ](#step-1-prepare-your-environment)

[ç¬¬äºŒæ­¥ï¼šä¸‹è½½æ¨¡å‹](#step-2-download-the-yi-model)

ç¬¬ä¸‰æ­¥ï¼šå¯åŠ¨ç½‘é¡µæœåŠ¡ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```bash
python demo/web_demo.py -c <ä½ çš„æ¨¡å‹è·¯å¾„>
```

å‘½ä»¤è¿è¡Œå®Œæ¯•åï¼Œä½ å¯ä»¥åœ¨æµè§ˆå™¨ä¸­è¾“å…¥æ§åˆ¶å°æä¾›çš„ç½‘å€ï¼Œæ¥ä½¿ç”¨ Web Demoã€‚

 ![å¿«é€Ÿä¸Šæ‰‹ - Web Demo](./assets/img/yi_34b_chat_web_demo.gif)

### å¿«é€Ÿä¸Šæ‰‹-LM-Studio
<details>
  <summary>AIä¸“ä¸šæœ¯è¯­ğŸ“šå­—å…¨éƒ½è®¤è¯†ä½†å°±æ˜¯çœ‹ä¸æ‡‚ğŸ¤”? æ¨¡å‹éƒ¨ç½²è¿è¡ŒåbugğŸ›ä¸æ–­? dockerå‘½ä»¤è¡Œæ“ä½œå®åœ¨æ˜¯å¤ªåäººç±»ğŸ˜©? å¦‚æœä½ æ­£åœ¨ä¸ºä¸Šè¿°é—®é¢˜å‘æ„ğŸ˜°, LM Studioå°†ä¼šæ˜¯ä½ æœ€å¥½çš„é€‰æ‹©âœ¨ğŸš€.</summary>

  #### æ­¥éª¤1ï¼šä¸‹è½½ä¸å®‰è£…
  å‰å¾€[LM Studioå®˜ç½‘](https://lmstudio.ai)ä¸‹è½½é€‚åˆæ‚¨æ“ä½œç³»ç»Ÿçš„LM Studioç‰ˆæœ¬ï¼Œå¹¶å®Œæˆå®‰è£…ã€‚
  
  #### æ­¥éª¤2ï¼šæ¨¡å‹é€‰æ‹©ä¸ä¸‹è½½
  å¯åŠ¨LM Studioåï¼Œåœ¨è½¯ä»¶å†…éƒ¨æœç´¢Yiç³»åˆ—æ¨¡å‹,å¹¶æ ¹æ®æ¨¡å‹ç³»ç»Ÿå…¼å®¹æ€§æ¨èä¸‹è½½ã€‚
  
  #### æ­¥éª¤3ï¼šé…ç½®æ¨¡å‹å‚æ•°
  æŒ‰ç…§LM Studioçš„æç¤ºé…ç½®æ¨¡å‹å‚æ•°ï¼Œç¡®ä¿ç¡¬ä»¶è®¾ç½®ä¸æ‚¨çš„ç³»ç»Ÿèµ„æºç›¸åŒ¹é…ã€‚
  
  #### æ­¥éª¤4ï¼šå¯åŠ¨æ¨¡å‹
  å®Œæˆé…ç½®åï¼Œæ‚¨å°±å¯ä»¥åœ¨æœ¬åœ°å¯åŠ¨Yiæ¨¡å‹ï¼Œå¹¶å¼€å§‹ä½¿ç”¨ã€‚
  
  LM StudioåŒæ—¶è¿˜æä¾›æ–¹ä¾¿å¿«æ·çš„å…¼å®¹OpenAIæ¥å£çš„å·¥å…·.å¯¹äºä¸ªäººæ—¥å¸¸åº”ç”¨,éƒ¨ç½²Yiæ¥è¿›è¡ŒèŠå¤©æˆ–é…ç½®æœåŠ¡å™¨ï¼ŒLM Studioå°†ä¼šæ˜¯ä½ å¾ˆå¥½çš„èµ·ç‚¹âœ¨ğŸš€ã€‚
</details>

### Finetuning

```bash
bash finetune/scripts/run_sft_Yi_6b.sh
```

Once finished, you can compare the finetuned model and the base model with the following command:

```bash
bash finetune/scripts/run_eval.sh
```
<details style="display: inline;"><summary>For advanced usage (like fine-tuning based on your custom data), see â¬‡ï¸</summary> <ul>

### Finetune code for Yi 6B and 34B

#### Preparation

##### From Image

By default, we use a small dataset from [BAAI/COIG](https://huggingface.co/datasets/BAAI/COIG) to finetune the base model.
You can also prepare your customized dataset in the following `jsonl` format:

```json
{ "prompt": "Human: Who are you? Assistant:", "chosen": "I'm Yi." }
```

And then mount them in the container to replace the default ones:

```bash
docker run -it \
    -v /path/to/save/finetuned/model/:/finetuned-model \
    -v /path/to/train.jsonl:/yi/finetune/data/train.json \
    -v /path/to/eval.jsonl:/yi/finetune/data/eval.json \
    ghcr.io/01-ai/yi:latest \
    bash finetune/scripts/run_sft_Yi_6b.sh
```

##### From Local Server

Make sure you have conda. If not, use

```bash
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
source ~/.bashrc
```

Then, create a conda env:

```bash
conda create -n dev_env python=3.10 -y
conda activate dev_env
pip install torch==2.0.1 deepspeed==0.10 tensorboard transformers datasets sentencepiece accelerate ray==2.7
```

#### Hardware Setup

For the Yi-6B model, a node with 4 GPUs, each has GPU mem larger than 60GB is recommended.

For the Yi-34B model, because the usage of zero-offload technique takes a lot CPU memory, please be careful to limit the GPU numbers in 34B finetune training. Please use CUDA_VISIBLE_DEVICES to limit the GPU number (as shown in scripts/run_sft_Yi_34b.sh).

A typical hardware setup for finetuning 34B model is a node with 8GPUS (limit to 4 in running by CUDA_VISIBLE_DEVICES=0,1,2,3), each has GPU mem larger than 80GB, with total CPU mem larger than 900GB.

#### Quick Start

Download a LLM-base model to MODEL_PATH (6B and 34B). A typical folder of models is like:

```bash
|-- $MODEL_PATH
|   |-- config.json
|   |-- pytorch_model-00001-of-00002.bin
|   |-- pytorch_model-00002-of-00002.bin
|   |-- pytorch_model.bin.index.json
|   |-- tokenizer_config.json
|   |-- tokenizer.model
|   |-- ...
```

Download a dataset from huggingface to local storage DATA_PATH, e.g. Dahoas/rm-static.

```bash
|-- $DATA_PATH
|   |-- data
|   |   |-- train-00000-of-00001-2a1df75c6bce91ab.parquet
|   |   |-- test-00000-of-00001-8c7c51afc6d45980.parquet
|   |-- dataset_infos.json
|   |-- README.md
```

`finetune/yi_example_dataset` has example datasets, which are modified from [BAAI/COIG](https://huggingface.co/datasets/BAAI/COIG)

```bash
|-- $DATA_PATH
    |--data
        |-- train.jsonl
        |-- eval.jsonl
```

`cd` into the scripts folder, copy and paste the script, and run. For example:

```bash
cd finetune/scripts

bash run_sft_Yi_6b.sh
```

For the Yi-6B base model, setting training_debug_steps=20 and num_train_epochs=4 can output a chat model, which takes about 20 minutes.

For the Yi-34B base model, it takes a relatively long time for initialization. Please be patient.

#### Evaluation

```bash
cd finetune/scripts

bash run_eval.sh
```

Then you'll see the answer from both the base model and the finetuned model
</ul>
</details>

### Quantization

#### GPT-Q
```bash
python quantization/gptq/quant_autogptq.py \
  --model /base_model                      \
  --output_dir /quantized_model            \
  --trust_remote_code
```

Once finished, you can then evaluate the resulting model as follows:

```bash
python quantization/gptq/eval_quantized_model.py \
  --model /quantized_model                       \
  --trust_remote_code
```

<details style="display: inline;"><summary>For a more detailed explanation, see â¬‡ï¸</summary> <ul>

#### GPT-Q quantization

[GPT-Q](https://github.com/IST-DASLab/gptq) is a PTQ(Post-Training Quantization)
method. It's memory saving and provides potential speedups while retaining the accuracy
of the model. 

Yi models can be GPT-Q quantized without a lot of efforts. 
We provide a step-by-step tutorial below.

To run GPT-Q, we will use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) and
[exllama](https://github.com/turboderp/exllama).
And the huggingface transformers has integrated optimum and auto-gptq to perform
GPTQ quantization on language models.

##### Do Quantization

The `quant_autogptq.py` script is provided for you to perform GPT-Q quantization:

```bash
python quant_autogptq.py --model /base_model \
    --output_dir /quantized_model --bits 4 --group_size 128 --trust_remote_code
```


##### Run Quantized Model

You can run a quantized model using the `eval_quantized_model.py`:

```bash
python eval_quantized_model.py --model /quantized_model --trust_remote_code
```
</ul>
</details>

#### AWQ
```bash
python quantization/awq/quant_autoawq.py \
  --model /base_model                      \
  --output_dir /quantized_model            \
  --trust_remote_code
```

Once finished, you can then evaluate the resulting model as follows:

```bash
python quantization/awq/eval_quantized_model.py \
  --model /quantized_model                       \
  --trust_remote_code
```
<details style="display: inline;"><summary>For detailed explanations, see â¬‡ï¸</summary> <ul>

#### AWQ quantization

[AWQ](https://github.com/mit-han-lab/llm-awq) is a PTQ(Post-Training Quantization)
method. It's an efficient and accurate low-bit weight quantization (INT3/4) for LLMs.

Yi models can be AWQ quantized without a lot of efforts. 
We provide a step-by-step tutorial below.

To run AWQ, we will use [AutoAWQ](https://github.com/casper-hansen/AutoAWQ).

##### Do Quantization

The `quant_autoawq.py` script is provided for you to perform AWQ quantization:

```bash
python quant_autoawq.py --model /base_model \
    --output_dir /quantized_model --bits 4 --group_size 128 --trust_remote_code
```

##### Run Quantized Model

You can run a quantized model using the `eval_quantized_model.py`:

```bash
python eval_quantized_model.py --model /quantized_model --trust_remote_code
```


</ul>
</details>
<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">Back to top â¬†ï¸ </a> ] </div>

### éƒ¨ç½²

å¦‚æœæ‚¨æƒ³éƒ¨ç½²Yiæ¨¡å‹ï¼Œè¯·ç¡®ä¿æ»¡è¶³ä»¥ä¸‹è½¯ä»¶å’Œç¡¬ä»¶è¦æ±‚ã€‚

#### è½¯ä»¶è¦æ±‚

åœ¨ä½¿ç”¨Yié‡åŒ–æ¨¡å‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…ä»¥ä¸‹åˆ—å‡ºçš„è½¯ä»¶ã€‚

| æ¨¡å‹ | è½¯ä»¶ |
|:---|:---|
Yi 4-bit quantized models | [AWQ and CUDA](https://github.com/casper-hansen/AutoAWQ?tab=readme-ov-file#install-from-pypi)
Yi 8-bit quantized models |  [GPTQ and CUDA](https://github.com/PanQiWei/AutoGPTQ?tab=readme-ov-file#quick-installation)

#### ç¡¬ä»¶è¦æ±‚

åœ¨æ‚¨çš„ç¯å¢ƒä¸­éƒ¨ç½²Yiä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¡¬ä»¶æ»¡è¶³ä»¥ä¸‹è¦æ±‚ã€‚

##### èŠå¤©æ¨¡å‹

| æ¨¡å‹                 | æœ€ä½æ˜¾å­˜      | æ¨èGPUç¤ºä¾‹                             |
|:----------------------|:--------------|:-------------------------------------:|
| Yi-6B-Chat           | 15 GB         | RTX 3090 <br> RTX 4090 <br>  A10 <br> A30             |
| Yi-6B-Chat-4bits     | 4 GB          | RTX 3060 <br>  RTX 4060                     |
| Yi-6B-Chat-8bits     | 8 GB          | RTX 3070 <br> RTX 4060                     |
| Yi-34B-Chat          | 72 GB         | 4 x RTX 4090 <br> A800 (80GB)               |
| Yi-34B-Chat-4bits    | 20 GB         | RTX 3090  <br> RTX 4090 <br> A10 <br> A30 <br> A100 (40GB) |
| Yi-34B-Chat-8bits    | 38 GB         | 2 x RTX 3090  <br> 2 x RTX 4090 <br> A800  (40GB) |

ä»¥ä¸‹æ˜¯ä¸åŒæ‰¹é‡ä½¿ç”¨æƒ…å†µä¸‹çš„è¯¦ç»†æœ€ä½æ˜¾å­˜è¦æ±‚ã€‚

|  æ¨¡å‹                  | batch=1 | batch=4 | batch=16 | batch=32 |
| :----------------------- | :------- | :------- | :-------- | :-------- |
| Yi-6B-Chat              | 12 GB   | 13 GB   | 15 GB    | 18 GB    |
| Yi-6B-Chat-4bits  | 4 GB    | 5 GB    | 7 GB     | 10 GB    |
| Yi-6B-Chat-8bits  | 7 GB    | 8 GB    | 10 GB    | 14 GB    |
| Yi-34B-Chat       | 65 GB   | 68 GB   | 76 GB    | > 80 GB   |
| Yi-34B-Chat-4bits | 19 GB   | 20 GB   | 30 GB    | 40 GB    |
| Yi-34B-Chat-8bits | 35 GB   | 37 GB   | 46 GB    | 58 GB    |

##### åŸºç¡€æ¨¡å‹

|æ¨¡å‹                   |æœ€ä½æ˜¾å­˜      |        æ¨èGPUç¤ºä¾‹                     |
|:----------------------|:--------------|:-------------------------------------:|
| Yi-6B                | 15 GB         | RTX3090 <br> RTX4090 <br> A10 <br> A30               |
| Yi-6B-200K           | 50 GB         | A800 (80 GB)                            |
| Yi-34B               | 72 GB         | 4 x RTX 4090 <br> A800 (80 GB)               |
| Yi-34B-200K          | 200 GB        | 4 x A800 (80 GB)                        |

### å­¦ä¹ ä¸­å¿ƒ

<details>
<summary> å¦‚æœä½ æƒ³å­¦ä¹ Yiï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å¤§é‡çš„æœ‰ç”¨çš„å­¦ä¹ èµ„æº â¬‡ï¸</summary>
<br>

æ¬¢è¿æ¥åˆ°Yiå­¦ä¹ ä¸­å¿ƒï¼

æ— è®ºä½ æ˜¯ç»éªŒä¸°å¯Œçš„å¼€å‘è€…è¿˜æ˜¯æ–°æ‰‹ï¼Œä½ éƒ½å¯ä»¥æ‰¾åˆ°å¤§é‡æœ‰ç”¨çš„å­¦ä¹ èµ„æºï¼Œä»¥æé«˜ä½ å¯¹Yiæ¨¡å‹çš„ç†è§£å’ŒæŠ€èƒ½ï¼ŒåŒ…æ‹¬æ·±å…¥çš„åšå®¢æ–‡ç« ã€å…¨é¢çš„è§†é¢‘æ•™ç¨‹ã€å®è·µæŒ‡å—ç­‰ç­‰ã€‚

åœ¨è¿™é‡Œï¼Œä½ èƒ½æ‰¾åˆ°çš„å†…å®¹æ˜¯ç”±çŸ¥è¯†æ¸Šåšçš„Yiä¸“å®¶å’Œçƒ­æƒ…çš„çˆ±å¥½è€…æ…·æ…¨è´¡çŒ®çš„ã€‚æˆ‘ä»¬å¯¹æ‚¨å®è´µçš„è´¡çŒ®è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼

åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿçƒ­çƒˆé‚€è¯·æ‚¨åŠ å…¥æˆ‘ä»¬çš„åä½œåŠªåŠ›ï¼Œä¸ºYiåšå‡ºè´¡çŒ®ã€‚å¦‚æœæ‚¨å·²ç»å¯¹Yiåšå‡ºäº†è´¡çŒ®ï¼Œè¯·ä¸è¦çŠ¹è±«ï¼Œåœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­å±•ç¤ºæ‚¨æ°å‡ºçš„å·¥ä½œã€‚

æœ‰äº†è¿™äº›å”¾æ‰‹å¯å¾—çš„èµ„æºï¼Œæ‚¨å°±å‡†å¤‡å¥½å¼€å§‹ä¸Yiçš„æ¿€åŠ¨äººå¿ƒçš„Yiå­¦ä¹ ä¹‹æ—…äº†ã€‚ç¥å­¦ä¹ æ„‰å¿«ï¼ğŸ¥³

#### æ•™ç¨‹


| ç±»å‹        | æ•™ç¨‹åœ°å€                                            |      æ—¥æœŸ      |     ä½œè€…     |
|:-------------|:--------------------------------------------------------|:----------------|:----------------|
| åšå®¢        | [æœ¬åœ°è¿è¡Œé›¶ä¸€ä¸‡ç‰© 34B å¤§æ¨¡å‹ï¼Œä½¿ç”¨ Llama.cpp & 21G æ˜¾å­˜](https://zhuanlan.zhihu.com/p/668921042)                  |  2023-11-26  |  [è‹æ´‹](https://github.com/soulteary)  |
| åšå®¢        | [Running Yi-34B-Chat locally using LlamaEdge](https://www.secondstate.io/articles/yi-34b/)                   |  2023-11-30  |  [Second State](https://github.com/second-state)  |
| åšå®¢       | [é›¶ä¸€ä¸‡ç‰©æ¨¡å‹æŠ˜è…¾ç¬”è®°ï¼šå®˜æ–¹ Yi-34B æ¨¡å‹åŸºç¡€ä½¿ç”¨](https://zhuanlan.zhihu.com/p/671387298)                           | 2023-12-10 |  [è‹æ´‹](https://github.com/soulteary)  |
| åšå®¢        | [CPU æ··åˆæ¨ç†ï¼Œéå¸¸è§å¤§æ¨¡å‹é‡åŒ–æ–¹æ¡ˆï¼šâ€œäºŒä¸‰äº”å…­â€ ä½é‡åŒ–æ–¹æ¡ˆ](https://zhuanlan.zhihu.com/p/671698216)                  | 2023-12-12 |  [è‹æ´‹](https://github.com/soulteary)  |
| è§†é¢‘       | [åªéœ€ 24G æ˜¾å­˜ï¼Œç”¨ vllm è·‘èµ·æ¥ Yi-34B ä¸­è‹±åŒè¯­å¤§æ¨¡å‹](https://www.bilibili.com/video/BV17t4y1f7Ee/)               | 2023-12-28 |  æ¼†å¦®å¦®  |
| è§†é¢‘       | [Install Yi 34B Locally - Chinese English Bilingual LLM](https://www.youtube.com/watch?v=CVQvj4Wrh4w&t=476s) | 2023-11-05  |  Fahd Mirza  |
</details>


# ğŸŸ¢ ä¸ºä»€ä¹ˆé€‰æ‹©Yiï¼Ÿ

  - [ğŸŒ ç”Ÿæ€ç³»ç»Ÿ](#-ç”Ÿæ€ç³»ç»Ÿ)
    - [ğŸ’¦ ä¸Šæ¸¸](#-ä¸Šæ¸¸)
    - [ğŸŒŠ ä¸‹æ¸¸](#-ä¸‹æ¸¸)
      - [ğŸ”— æœåŠ¡](#-æœåŠ¡)
      - [âš™ï¸ é‡åŒ–](#ï¸-é‡åŒ–)
      - [ğŸ› ï¸ å¾®è°ƒ](#ï¸-å¾®è°ƒ)
      - [API](#api)
  - [ğŸ“Œ åŸºå‡†æµ‹è¯•](#-åŸºå‡†æµ‹è¯•)
    - [ğŸ“Š èŠå¤©æ¨¡å‹æ€§èƒ½](#-èŠå¤©æ¨¡å‹æ€§èƒ½)
    - [ğŸ“Š åŸºç¡€æ¨¡å‹æ€§èƒ½](#-åŸºç¡€æ¨¡å‹æ€§èƒ½)

## ğŸŒ ç”Ÿæ€ç³»ç»Ÿ

Yiæ‹¥æœ‰ä¸€ä¸ªå…¨é¢çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæä¾›ä¸€ç³»åˆ—å·¥å…·ã€æœåŠ¡å’Œæ¨¡å‹ï¼Œä»¥ä¸°å¯Œæ‚¨çš„ä½“éªŒå¹¶æœ€å¤§åŒ–ç”Ÿäº§åŠ›ã€‚

- [ğŸ’¦ ä¸Šæ¸¸](#-ä¸Šæ¸¸)
- [ğŸŒŠ ä¸‹æ¸¸](#-ä¸‹æ¸¸)
  - [ğŸ”— æœåŠ¡](#-æœåŠ¡)
  - [âš™ï¸ é‡åŒ–](#ï¸-é‡åŒ–)
  - [ğŸ› ï¸ å¾®è°ƒ](#ï¸-å¾®è°ƒ)
  - [API](#api)

### ğŸ’¦ ä¸Šæ¸¸

Yiç³»åˆ—æ¨¡å‹éµå¾ªä¸LLaMAç›¸åŒçš„æ¨¡å‹æ¶æ„ã€‚é€‰æ‹©Yiï¼Œæ‚¨å¯ä»¥åˆ©ç”¨LLaMAç”Ÿæ€ç³»ç»Ÿä¸­ç°æœ‰çš„å·¥å…·ã€åº“å’Œèµ„æºï¼Œæ— éœ€åˆ›å»ºæ–°å·¥å…·ï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚

ä¾‹å¦‚ï¼ŒYiç³»åˆ—æ¨¡å‹ä»¥LLaMAæ¨¡å‹çš„æ ¼å¼ä¿å­˜ã€‚æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨`LLaMAForCausalLM`å’Œ`LLaMATokenizer`åŠ è½½æ¨¡å‹ã€‚æ›´å¤šä¿¡æ¯ï¼Œè¯·è§[ä½¿ç”¨èŠå¤©æ¨¡å‹](#31-ä½¿ç”¨èŠå¤©æ¨¡å‹)ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("01-ai/Yi-34b", use_fast=False)

model = AutoModelForCausalLM.from_pretrained("01-ai/Yi-34b", device_map="auto")
```

### ğŸŒŠ ä¸‹æ¸¸

> ğŸ’¡ æç¤º
> 
> - éšæ—¶åˆ›å»ºPRå¹¶åˆ†äº«æ‚¨ä½¿ç”¨Yiç³»åˆ—æ¨¡å‹æ„å»ºçš„å‡ºè‰²ä½œå“ã€‚
>
> - ä¸ºäº†å¸®åŠ©ä»–äººå¿«é€Ÿç†è§£æ‚¨çš„å·¥ä½œï¼Œå»ºè®®ä½¿ç”¨`<æ¨¡å‹åç§°>: <æ¨¡å‹ç®€ä»‹> + <æ¨¡å‹äº®ç‚¹>`çš„æ ¼å¼ã€‚

#### ğŸ”— æœåŠ¡

å¦‚æœæ‚¨æƒ³åœ¨å‡ åˆ†é’Ÿå†…å¼€å§‹ä½¿ç”¨Yiï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹åŸºäºYiæ„å»ºçš„æœåŠ¡ã€‚

- Yi-34B-Chatï¼šæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å¹³å°ä¸Yiè¿›è¡ŒèŠå¤©ï¼š
  - [Yi-34B-Chat | Hugging Face](https://huggingface.co/spaces/01-ai/Yi-34B-Chat)
  - [Yi-34B-Chat | Yi Platform](https://platform.lingyiwanwu.com/)ï¼š**æ³¨æ„**ç›®å‰ä»…é€šè¿‡ç™½åå•æä¾›ã€‚æ¬¢è¿ç”³è¯·ï¼ˆå¡«å†™[è‹±æ–‡](https://cn.mikecrm.com/l91ODJf)æˆ–[ä¸­æ–‡](https://cn.mikecrm.com/gnEZjiQ)è¡¨æ ¼ï¼‰å¹¶äº²èº«ä½“éªŒï¼

- [Yi-6B-Chat (Replicate)](https://replicate.com/01-ai)ï¼šæ‚¨å¯ä»¥é€šè¿‡è®¾ç½®é¢å¤–å‚æ•°å’Œè°ƒç”¨APIsï¼Œä»¥æ›´å¤šé€‰é¡¹ä½¿ç”¨æ­¤æ¨¡å‹ã€‚

- [ScaleLLM](https://github.com/vectorch-ai/ScaleLLM#supported-models)ï¼šæ‚¨å¯ä»¥ä½¿ç”¨æ­¤æœåŠ¡åœ¨æœ¬åœ°è¿è¡ŒYiæ¨¡å‹ï¼Œå¢åŠ çµæ´»æ€§å’Œå®šåˆ¶æ€§ã€‚

#### âš™ï¸ é‡åŒ–

å¦‚æœæ‚¨çš„è®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Yiçš„é‡åŒ–æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

è¿™äº›é‡åŒ–æ¨¡å‹è™½ç„¶ç²¾åº¦é™ä½ï¼Œä½†æä¾›äº†æ›´é«˜çš„æ•ˆç‡ï¼Œä¾‹å¦‚æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´å°çš„RAMä½¿ç”¨é‡ã€‚

- [TheBloke/Yi-34B-GPTQ](https://huggingface.co/TheBloke/Yi-34B-GPTQ)
- [TheBloke/Yi-34B-GGUF](https://huggingface.co/TheBloke/Yi-34B-GGUF)
- [TheBloke/Yi-34B-AWQ](https://huggingface.co/TheBloke/Yi-34B-AWQ)

#### ğŸ› ï¸ å¾®è°ƒ

å¦‚æœæ‚¨å¸Œæœ›æ¢ç´¢Yiåºå¤§å®¶æ—ä¸­çš„å¤šæ ·åŒ–èƒ½åŠ›ï¼Œæ‚¨å¯ä»¥æ·±å…¥äº†è§£ä¸‹é¢çš„Yiå¾®è°ƒæ¨¡å‹ã€‚

- [TheBloke æ¨¡å‹](https://huggingface.co/TheBloke)ï¼šè¿™ä¸ªç½‘ç«™æ‰˜ç®¡äº†è®¸å¤šä»åŒ…æ‹¬Yiåœ¨å†…çš„å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ´¾ç”Ÿçš„å¾®è°ƒæ¨¡å‹ã€‚
  
  è¿™ä¸æ˜¯Yiçš„è¯¦å°½åˆ—è¡¨ï¼Œä½†ä»¥ä¸‹æ˜¯åŸºäºä¸‹è½½é‡æ’åºçš„å‡ ä¸ªä¾‹å­ï¼š
  - [TheBloke/dolphin-2_2-yi-34b-AWQ](https://huggingface.co/TheBloke/dolphin-2_2-yi-34b-AWQ)
  - [TheBloke/Yi-34B-Chat-AWQ](https://huggingface.co/TheBloke/Yi-34B-Chat-AWQ)
  - [TheBloke/Yi-34B-Chat-GPTQ](https://huggingface.co/TheBloke/Yi-34B-Chat-GPTQ)
  
- [SUSTech/SUS-Chat-34B](https://huggingface.co/SUSTech/SUS-Chat-34B)ï¼šè¿™ä¸ªæ¨¡å‹åœ¨æ‰€æœ‰70Bä»¥ä¸‹çš„æ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼Œå¹¶ä¸”è¶…è¶Šäº†ä½“é‡æ˜¯å…¶ä¸¤å€çš„deepseek-llm-67b-chatã€‚æ‚¨å¯ä»¥åœ¨[å¼€æ”¾LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸ŠæŸ¥çœ‹ç»“æœã€‚
  
- [OrionStarAI/OrionStar-Yi-34B-Chat-Llama](https://huggingface.co/OrionStarAI/OrionStar-Yi-34B-Chat-Llama)ï¼šè¿™ä¸ªæ¨¡å‹åœ¨C-Evalå’ŒCMMLUè¯„ä¼°ä¸­è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ï¼ˆå¦‚GPT-4, Qwen-14B-Chat, Baichuan2-13B-Chatï¼‰, åœ¨[OpenCompass LLM æ’è¡Œæ¦œ](https://opencompass.org.cn/leaderboard-llm)ä¸Šè¡¨ç°å‡ºè‰²ã€‚
  
- [NousResearch/Nous-Capybara-34B](https://huggingface.co/NousResearch/Nous-Capybara-34B)ï¼šè¿™ä¸ªæ¨¡å‹åœ¨Capybaraæ•°æ®é›†ä¸Šä½¿ç”¨200Kä¸Šä¸‹æ–‡é•¿åº¦å’Œ3ä¸ªè®­ç»ƒå‘¨æœŸè¿›è¡Œè®­ç»ƒã€‚

#### API

- [amazing-openai-api](https://github.com/soulteary/amazing-openai-api)ï¼šè¿™ä¸ªå·¥å…·å¯ä»¥å°†Yiæ¨¡å‹APIè½¬æ¢æˆOpenAI APIæ ¼å¼ã€‚
- [LlamaEdge](https://www.secondstate.io/articles/yi-34b/#create-an-openai-compatible-api-service-for-the-yi-34b-chat-model)ï¼šè¿™ä¸ªå·¥å…·ä½¿ç”¨å¯ç§»æ¤çš„Wasmï¼ˆWebAssemblyï¼‰æ–‡ä»¶æ„å»ºäº†ä¸€ä¸ªä¸OpenAIå…¼å®¹çš„APIæœåŠ¡å™¨ï¼Œç”¨äºYi-34B-Chatï¼Œç”±Rusté©±åŠ¨ã€‚

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">å›åˆ°é¡¶éƒ¨ â¬†ï¸ </a> ] </div>

## ğŸ“Œ Benchmarks 

- [ğŸ“Š Chat model performance](#-chat-model-performance)
- [ğŸ“Š Base model performance](#-base-model-performance)

### ğŸ“Š Chat model performance

Yi-34B-Chat model demonstrates exceptional performance, ranking first among all existing open-source models in the benchmarks including MMLU, CMMLU, BBH, GSM8k, and more.

![Chat model performance](./assets/img/benchmark_chat.png) 

<details>
<summary> Evaluation methods and challenges â¬‡ï¸ </summary>

- **Evaluation methods**: we evaluated various benchmarks using both zero-shot and few-shot methods, except for TruthfulQA.
- **Zero-shot vs. few-shot**: in chat models, the zero-shot approach is more commonly employed.
- **Evaluation strategy**: our evaluation strategy involves generating responses while following instructions explicitly or implicitly (such as using few-shot examples). We then isolate relevant answers from the generated text.
- **Challenges faced**: some models are not well-suited to produce output in the specific format required by instructions in few datasets, which leads to suboptimal results.

<strong>*</strong>: C-Eval results are evaluated on the validation datasets
</details>

### ğŸ“Š Base model performance

The Yi-34B and Yi-34B-200K models stand out as the top performers among open-source models, especially excelling in MMLU, CMML, common-sense reasoning, reading comprehension, and more.

![Base model performance](./assets/img/benchmark_base.png)

<details>
<summary> Evaluation methods â¬‡ï¸</summary>

- **Disparity in Results**: while benchmarking open-source models, a disparity has been noted between results from our pipeline and those reported by public sources like OpenCompass.
- **Investigation Findings**: a deeper investigation reveals that variations in prompts, post-processing strategies, and sampling techniques across models may lead to significant outcome differences.
- **Uniform Benchmarking Process**: our methodology aligns with the original benchmarksâ€”consistent prompts and post-processing strategies are used, and greedy decoding is applied during evaluations without any post-processing for the generated content.
- **Efforts to Retrieve Unreported Scores**: for scores that were not reported by the original authors (including scores reported with different settings), we try to get results with our pipeline.
- **Extensive Model Evaluation**: to evaluate the modelâ€™s capability extensively, we adopted the methodology outlined in Llama2. Specifically, we included PIQA, SIQA, HellaSwag, WinoGrande, ARC, OBQA, and CSQA to assess common sense reasoning. SquAD, QuAC, and BoolQ were incorporated to evaluate reading comprehension.
- **Special Configurations**: CSQA was exclusively tested using a 7-shot setup, while all other tests were conducted with a 0-shot configuration. Additionally, we introduced GSM8K (8-shot@1), MATH (4-shot@1), HumanEval (0-shot@1), and MBPP (3-shot@1) under the category "Math & Code".
- **Falcon-180B Caveat**: Falcon-180B was not tested on QuAC and OBQA due to technical constraints. Its performance score is an average from other tasks, and considering the generally lower scores of these two tasks, Falcon-180B's capabilities are likely not underestimated.
</details>

# ğŸŸ¢ Who can use Yi?

Everyone! ğŸ™Œ âœ…

- The Yi series models are free for personal usage, academic purposes, and commercial use. All usage must adhere to the [Yi Series Models Community License Agreement 2.1](https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt)
  
- For free commercial use, you only need to [complete this form](https://www.lingyiwanwu.com/yi-license) to get a Yi Model Commercial License.

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">Back to top â¬†ï¸ </a> ] </div>

# ğŸŸ¢ Misc.

### Acknowledgments

A heartfelt thank you to each of you who have made contributions to the Yi community! You have helped Yi not just a project, but a vibrant, growing home for innovation.

<!---
ref https://github.com/ngryman/contributor-faces
npx contributor-faces --exclude "*bot*" --limit 70 --repo "https://github.com/01-ai/Yi"

change the height and width for each of the contributors from 80 to 50 at ref index.js.
--->

[//]: contributor-faces
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/ZhaoFancy"><img style="margin:0" src="https://avatars.githubusercontent.com/u/139539780?v=4" title="ZhaoFancy" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/Anonymitaet"><img style="margin:0" src="https://avatars.githubusercontent.com/u/50226895?v=4" title="Anonymitaet" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/findmyway"><img style="margin:0" src="https://avatars.githubusercontent.com/u/5612003?v=4" title="findmyway" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/shiyue-loop"><img style="margin:0" src="https://avatars.githubusercontent.com/u/150643331?v=4" title="shiyue-loop" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/richardllin"><img style="margin:0" src="https://avatars.githubusercontent.com/u/1932744?v=4" title="richardllin" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/jiangchengSilent"><img style="margin:0" src="https://avatars.githubusercontent.com/u/143983063?v=4" title="jiangchengSilent" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/loofahcus"><img style="margin:0" src="https://avatars.githubusercontent.com/u/15729967?v=4" title="loofahcus" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/Yimi81"><img style="margin:0" src="https://avatars.githubusercontent.com/u/66633207?v=4" title="Yimi81" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/ly-nld"><img style="margin:0" src="https://avatars.githubusercontent.com/u/38471793?v=4" title="ly-nld" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/WayTooWill"><img style="margin:0" src="https://avatars.githubusercontent.com/u/119883899?v=4" title="WayTooWill" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/kai01ai"><img style="margin:0" src="https://avatars.githubusercontent.com/u/140378742?v=4" title="kai01ai" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/forpanyang"><img style="margin:0" src="https://avatars.githubusercontent.com/u/138085590?v=4" title="forpanyang" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/0x1111"><img style="margin:0" src="https://avatars.githubusercontent.com/u/750392?v=4" title="0x1111" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/angeligareta"><img style="margin:0" src="https://avatars.githubusercontent.com/u/32129522?v=4" title="angeligareta" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/xffxff"><img style="margin:0" src="https://avatars.githubusercontent.com/u/30254428?v=4" title="xffxff" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/tpoisonooo"><img style="margin:0" src="https://avatars.githubusercontent.com/u/7872421?v=4" title="tpoisonooo" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/tdolan21"><img style="margin:0" src="https://avatars.githubusercontent.com/u/40906019?v=4" title="tdolan21" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/statelesshz"><img style="margin:0" src="https://avatars.githubusercontent.com/u/28150734?v=4" title="statelesshz" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/renxiaoyi"><img style="margin:0" src="https://avatars.githubusercontent.com/u/10918916?v=4" title="renxiaoyi" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/markli404"><img style="margin:0" src="https://avatars.githubusercontent.com/u/116385770?v=4" title="markli404" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/fecet"><img style="margin:0" src="https://avatars.githubusercontent.com/u/41792945?v=4" title="fecet" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/cArlIcon"><img style="margin:0" src="https://avatars.githubusercontent.com/u/7384654?v=4" title="cArlIcon" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/alabulei1"><img style="margin:0" src="https://avatars.githubusercontent.com/u/45785633?v=4" title="alabulei1" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/eltociear"><img style="margin:0" src="https://avatars.githubusercontent.com/u/22633385?v=4" title="eltociear" width="50" height="50"></a>
<a style="display:inline-block;width=50px;height=50px" href="https://github.com/Gmgge"><img style="margin:0" src="https://avatars.githubusercontent.com/u/48548141?v=4" title="Gmgge" width="50" height="50"></a>

[//]: contributor-faces

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">Back to top â¬†ï¸ </a> ] </div>

### ğŸ“¡ Disclaimer

We use data compliance checking algorithms during the training process, to
ensure the compliance of the trained model to the best of our ability. Due to
complex data and the diversity of language model usage scenarios, we cannot
guarantee that the model will generate correct, and reasonable output in all
scenarios. Please be aware that there is still a risk of the model producing
problematic outputs. We will not be responsible for any risks and issues
resulting from misuse, misguidance, illegal usage, and related misinformation,
as well as any associated data security concerns.

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">Back to top â¬†ï¸ </a> ] </div>

### ğŸªª License

The source code in this repo is licensed under the [Apache 2.0
license](https://github.com/01-ai/Yi/blob/main/LICENSE). The Yi series models are fully open for academic research and free for commercial use, with automatic permission granted upon application. All usage must adhere to the [Yi Series Models Community License Agreement 2.1](https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt).
For free commercial use, you only need to send an email to [get official commercial permission](https://www.lingyiwanwu.com/yi-license).

<div align="right"> [ <a href="#building-the-next-generation-of-open-source-and-bilingual-llms">Back to top â¬†ï¸ </a> ] </div>
