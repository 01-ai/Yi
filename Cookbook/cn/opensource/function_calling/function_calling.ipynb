{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸåŠ¨æ‰‹å®è·µfunction call\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Yi-1.5-9B-Chatæ¨¡å‹æ¥å®ç°ä¸€ä¸ªç‹¬ç«‹çš„function callç¤ºä¾‹ã€‚è¿™ä¸ªç¤ºä¾‹ä¸ä¾èµ–äºä»»ä½•ç‰¹å®šçš„æ¡†æ¶ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨Hugging Faceçš„transformersåº“æ¥åŠ è½½å’Œä½¿ç”¨æ¨¡å‹ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š\n",
    "âš ï¸è¿™é‡Œè¯·æ³¨æ„ä½ çš„ç”µè„‘æ˜¾å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install transformers torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹é€æ­¥æ„å»ºæˆ‘ä»¬çš„function callå®ç°ã€‚\n",
    "å’Œä¸Šé¢ä¸€æ ·æˆ‘ä»¬ä¹Ÿä½¿ç”¨åŠ å‡ä¹˜ä¸‰ä¸ªå‡½æ•°æ¥åšç¤ºä¾‹\n",
    "#### æ­¥éª¤1ï¼šå¯¼å…¥å¿…è¦çš„åº“å’Œå®šä¹‰å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# å®šä¹‰å¯ç”¨çš„å‡½æ•°\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "def plus(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "def minus(a: int, b: int) -> int:\n",
    "    return a - b\n",
    "# åœ¨è¿™é‡Œä½ å¯ä»¥æ·»åŠ è‡ªå·±éœ€è¦çš„å‡½æ•°\n",
    "# å‡½æ•°æ˜ å°„\n",
    "available_functions = {\n",
    "    \"multiply\": multiply,\n",
    "    \"plus\": plus,\n",
    "    \"minus\": minus\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ­¥éª¤2ï¼šåŠ è½½Yiæ¨¡å‹å’Œåˆ†è¯å™¨(è¿™ä¸€æ­¥æˆ‘ä»¬ä½¿ç”¨transformersè¿›è¡ŒåŠ è½½)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# åŠ è½½Yiæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "model_path = \"01-ai/Yi-1.5-9B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ­¥éª¤3ï¼šå®ç°ç”Ÿæˆå“åº”å’Œè§£æå‡½æ•°è°ƒç”¨çš„åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ç”ŸæˆYiçš„å›å¤\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, temperature=0.7, top_p=0.95)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"Human:\")[0].strip()\n",
    "\n",
    "# è§£æè¾“å‡ºï¼Œæå–å‡½æ•°ä¿¡æ¯\n",
    "def parse_function_call(response):\n",
    "    try:\n",
    "        # å°è¯•ä»å›å¤ä¸­æå–JSONæ ¼å¼çš„å‡½æ•°è°ƒç”¨\n",
    "        start = response.index(\"{\")\n",
    "        end = response.rindex(\"}\") + 1\n",
    "        function_call_json = response[start:end]\n",
    "        function_call = json.loads(function_call_json)\n",
    "        return function_call\n",
    "    except (ValueError, json.JSONDecodeError):\n",
    "        return None\n",
    "\n",
    "# æ‰§è¡Œå‡½æ•°è°ƒç”¨(å‡½æ•°ä¿¡æ¯ä¼ é€’çš„æ¡¥æ¢)\n",
    "def execute_function(function_name: str, arguments: dict) -> Any:\n",
    "    if function_name in available_functions:\n",
    "        return available_functions[function_name](**arguments)\n",
    "    else:\n",
    "        raise ValueError(f\"Function {function_name} not found\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ­¥éª¤4ï¼šå®ç°ä¸»å¾ªç¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ä¸»å¾ªç¯\n",
    "def main():\n",
    "    # è¿™ä¸ªæç¤ºè¯æ¨¡ç‰ˆä¸­æœ‰å‡½æ•°çš„ä¿¡æ¯ï¼Œå¦‚æœä½ éœ€è¦åŠ å…¶å®ƒçš„å‡½æ•°ï¼Œè¿™é‡Œä¹Ÿè¦å’Œå¤§æ¨¡å‹åŒæ­¥\n",
    "    system_prompt = \"\"\"You are an AI assistant capable of calling functions to perform tasks. When a user asks a question that requires calling a function, respond with a JSON object containing the function name and arguments. Available functions are:\n",
    "    - multiply(a: int, b: int) -> int: Multiplies two integers\n",
    "    - plus(a: int, b: int) -> int: Adds two integers\n",
    "    - minus(a: int, b: int) -> int: Subtracts two integers\n",
    "    For example, if the user asks \"What is 5 plus 3?\", respond with:\n",
    "    {\"function\": \"plus\", \"arguments\": {\"a\": 5, \"b\": 3}}\n",
    "    If no function call is needed, respond normally.\"\"\"\n",
    "\n",
    "    conversation_history = [f\"System: {system_prompt}\"]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Human: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²\n",
    "        conversation_history.append(f\"Human: {user_input}\")\n",
    "        \n",
    "        # æ„å»ºå®Œæ•´çš„æç¤º\n",
    "        full_prompt = \"\\n\".join(conversation_history) + \"\\nAssistant:\"\n",
    "\n",
    "        # è·å–æ¨¡å‹å“åº”\n",
    "        response = generate_response(full_prompt)\n",
    "        print(f\"Model response: {response}\")\n",
    "\n",
    "        # è§£æå¯èƒ½çš„å‡½æ•°è°ƒç”¨\n",
    "        function_call = parse_function_call(response)\n",
    "\n",
    "        if function_call:\n",
    "            function_name = function_call[\"function\"]\n",
    "            arguments = function_call[\"arguments\"]\n",
    "\n",
    "            try:\n",
    "                # æ‰§è¡Œå‡½æ•°\n",
    "                result = execute_function(function_name, arguments)\n",
    "\n",
    "                # å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²\n",
    "                conversation_history.append(f\"Assistant: The result of {function_name}({arguments}) is {result}\")\n",
    "                print(f\"Assistant: The result of {function_name}({arguments}) is {result}\")\n",
    "            except Exception as e:\n",
    "                error_message = f\"An error occurred: {str(e)}\"\n",
    "                conversation_history.append(f\"Assistant: {error_message}\")\n",
    "                print(f\"Assistant: {error_message}\")\n",
    "        else:\n",
    "            # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œç›´æ¥å°†æ¨¡å‹çš„å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²\n",
    "            conversation_history.append(f\"Assistant: {response}\")\n",
    "            print(f\"Assistant: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¿è¡Œç¤ºä¾‹\n",
    "\n",
    "è¦è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œä½ éœ€è¦ç¡®ä¿å·²ç»ä¸‹è½½äº†Yi-1.5-9B-Chatæ¨¡å‹ï¼Œæˆ–è€…å°†`model_path`æ›´æ”¹ä¸ºæ¨¡å‹çš„å®é™…è·¯å¾„ã€‚ç„¶åï¼Œä½ å¯ä»¥ç›´æ¥è¿è¡Œè¿™ä¸ªPythonè„šæœ¬ã€‚\n",
    "\n",
    "ä½¿ç”¨ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Human: What is 5 plus 3?\n",
    "Model response: Here's the function call to perform the addition:\n",
    "{\"function\": \"plus\", \"arguments\": {\"a\": 5, \"b\": 3}}\n",
    "Assistant: The result of plus({'a': 5, 'b': 3}) is 8"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
