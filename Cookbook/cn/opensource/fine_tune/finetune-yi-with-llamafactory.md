### ğŸŒŸä½¿ç”¨LLaMA-Factoryå¾®è°ƒ

LLaMA Factoryæ˜¯ä¸€æ¬¾å¼€æºä½ä»£ç å¤§æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œé›†æˆäº†ä¸šç•Œå¹¿æ³›ä½¿ç”¨çš„å¾®è°ƒæŠ€æœ¯ï¼Œæ˜¯åŒ—èˆªçš„åšå£«ç”Ÿéƒ‘è€€å¨çš„æ°ä½œã€‚å¾®è°ƒçš„è¿‡ç¨‹å¾ˆæ–¹ä¾¿ï¼Œè·Ÿç€æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥!

#### å®‰è£…

é¦–å…ˆæˆ‘ä»¬æ‹‰å–LLaMA-Factoryåˆ°æœ¬åœ°ï¼š

``````
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
``````

å®‰è£…ä¾èµ–ï¼š

``````
# âš ï¸ä¸‹é¢ä¸¤è¡Œå‘½ä»¤å»ç»ˆç«¯æ‰§è¡Œâš ï¸
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
``````

å¦‚æœä½ è¿˜æ²¡æœ‰ä¸‹è½½yiæ¨¡å‹å»ºè®®ä»**Huggingface**æˆ–è€…**ModelScope**ä¸­ä¸‹è½½å¯¹åº”çš„ä»£ç å¦‚ä¸‹ï¼š

``````
# ä»ModelScopeä¸­ä¸‹è½½
git clone https://www.modelscope.cn/01ai/Yi-1.5-6B-Chat.git 
# ä»Huggingfaceä¸‹è½½
git clone https://huggingface.co/01-ai/Yi-1.5-6B-Chat
``````

#### å¼€å§‹å¾®è°ƒ

1. åˆ›å»ºå¾®è°ƒè®­ç»ƒç›¸å…³çš„é…ç½®æ–‡ä»¶ã€‚

   - åœ¨Llama-Factoryçš„æ–‡ä»¶å¤¹é‡Œï¼Œæ‰“å¼€examples\train_qloraä¸‹æä¾›çš„`llama3_lora_sft_``awq``.yaml`ï¼Œå¤åˆ¶ä¸€ä»½å¹¶é‡å‘½åä¸º`yi_lora_sft_bitsandbytes.yaml`ã€‚

   - è¿™ä¸ªæ–‡ä»¶é‡Œé¢å†™ç€å’Œå¾®è°ƒç›¸å…³çš„å…³é”®å‚æ•°ï¼šæ¯”å¦‚ä½¿ç”¨å“ªä¸ªæ¨¡å‹ï¼Ÿè¿›è¡Œä»€ä¹ˆæ ·çš„å‹ç¼©é‡åŒ–ï¼Ÿä½¿ç”¨ä»€ä¹ˆæ•°æ®é›†ï¼ˆè¿™é‡Œæ˜¯identityï¼‰ï¼Ÿè¿™ä¸ªæ•°æ®é›†å­¦ä¹ å‡ éï¼ˆnum_train_epochsï¼‰ï¼Ÿå¾®è°ƒåçš„æ¨¡å‹æƒé‡ä¿å­˜åœ¨å“ªé‡Œï¼Ÿ

2. `yi_lora_sft_bitsandbytes.yaml`çš„å†…å®¹å¡«å……ä¸ºï¼š

   ``````
   ### model
   model_name_or_path: <ä½ ä¸‹è½½çš„æ¨¡å‹ä½ç½®ï¼Œä¸è¦å¸¦æ‹¬å·ï¼Œæ¯”å¦‚æˆ‘å†™äº†../Yi-1.5-6B-Chat>
   quantization_bit: 4
   
   ### method
   stage: sft
   do_train: true
   finetuning_type: lora
   lora_target: all
   
   ### dataset
   dataset: identity
   template: yi
   cutoff_len: 1024
   max_samples: 1000
   overwrite_cache: true
   preprocessing_num_workers: 16
   
   ### output
   output_dir: saves/yi-6b/lora/sft
   logging_steps: 10
   save_steps: 500
   plot_loss: true
   overwrite_output_dir: true
   
   ### train
   per_device_train_batch_size: 1
   gradient_accumulation_steps: 8
   learning_rate: 1.0e-4
   num_train_epochs: 3.0
   lr_scheduler_type: cosine
   warmup_ratio: 0.1
   fp16: true
   
   ### eval
   val_size: 0.1
   per_device_eval_batch_size: 1
   eval_strategy: steps
   eval_steps: 500
   ``````

   è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„identityæ•°æ®é›†ï¼Œä¿—è¯è¯´å°±æ˜¯â€œè‡ªæˆ‘è®¤çŸ¥â€æ•°æ®é›†ï¼Œä¹Ÿå°±æ˜¯è¯´å½“ä½ é—®æ¨¡å‹â€œä½ å¥½ä½ æ˜¯è°â€çš„æ—¶å€™ï¼Œæ¨¡å‹ä¼šå‘Šè¯‰ä½ æˆ‘å«nameç”±authorå¼€å‘ã€‚å¦‚æœä½ æŠŠæ•°æ®é›†æ›´æ”¹æˆä½ è‡ªå·±çš„åå­—ï¼Œé‚£ä½ å°±å¯ä»¥å¾®è°ƒä¸€ä¸ªå±äºä½ è‡ªå·±çš„å¤§æ¨¡å‹å•¦ã€‚

3. æ‰“å¼€ç»ˆç«¯terminalï¼Œè¾“å…¥ä»¥ä¸‹å‘½ä»¤å¯åŠ¨å¾®è°ƒè„šæœ¬(å¤§æ¦‚éœ€è¦10åˆ†é’Ÿ)ï¼š

   ``````bash
   llamafactory-cli train examples/train_qlora/yi_lora_sft_bitsandbytes.yaml
   ``````

#### æ¨ç†æµ‹è¯•

1. è¯·å‚è€ƒLlama-Factoryæ–‡ä»¶å¤¹ä¸­ï¼Œexamples\inferenceä¸‹æä¾›çš„`llama3_lora_sft.yaml`ï¼Œå¤åˆ¶ä¸€ä»½ï¼Œå¹¶é‡å‘½åä¸º`yi_lora_sft.yaml`ã€‚

  å†…å®¹å¡«å……ä¸ºï¼š

  ``````
  model_name_or_path: <å’Œä¹‹å‰ä¸€æ ·ï¼Œä½ ä¸‹è½½çš„æ¨¡å‹ä½ç½®ï¼Œæ¯”å¦‚æˆ‘å†™äº†../Yi-1.5-6B-Chat>
  adapter_name_or_path: saves/yi-6b/lora/sft
  template: yi
  finetuning_type: lora
  ``````

2. å›åˆ°åˆšåˆšç»“æŸå¾®è°ƒçš„ç»ˆç«¯Terminalï¼Œè¿è¡Œä¸‹é¢çš„æ¨ç†å‘½ä»¤ï¼š
``````
llamafactory-cli chat examples/inference/yi_lora_sft.yaml
``````

å¥½å•¦ï¼Œä½¿ç”¨llamafactoryå¾®è°ƒYiæ¨¡å‹çš„æ•™ç¨‹å°±ç»“æŸå•¦ï¼Œæ˜¯ä¸æ˜¯æ„Ÿè§‰ç‰¹åˆ«æœ‰æˆå°±æ„Ÿï¼Œæ¬¢è¿ç»§ç»­æŸ¥çœ‹æˆ‘ä»¬å…¶å®ƒçš„æ•™ç¨‹å™¢ã€‚