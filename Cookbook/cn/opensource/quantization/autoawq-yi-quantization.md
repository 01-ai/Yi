### ğŸŒŸä½¿ç”¨AutoAWQé‡åŒ–

AutoAWQ æ˜¯ä¸€æ¬¾æ˜“äºä½¿ç”¨çš„ 4 ä½é‡åŒ–æ¨¡å‹è½¯ä»¶åŒ…ã€‚ä¸ FP16 ç›¸æ¯”ï¼ŒAutoAWQ å¯å°†æ¨¡å‹é€Ÿåº¦æé«˜ 3 å€å¹¶å°†å†…å­˜éœ€æ±‚é™ä½ 3 å€ã€‚

AutoAWQ å®ç°äº†æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ– (AWQ) ç®—æ³•æ¥é‡åŒ– LLMã€‚

æ­¤æ¬¡æ‰€æœ‰æ¼”ç¤ºï¼Œå‡é‡‡ç”¨Yi-1.5-6B-Chatä½œä¸ºç¤ºä¾‹ã€‚

ä¸‹é¢æ˜¯æ­¤æ¬¡æ¼”ç¤ºæ˜¾å­˜å’Œç¡¬ç›˜å ç”¨æƒ…å†µï¼š

| æ¨¡å‹ | æ˜¾å­˜ä½¿ç”¨ | ç¡¬ç›˜å ç”¨  |
|--|------|-------|
| Yi-1.5-6B-Chat | 6G   | 24.5G |

#### å®‰è£…
AWQçš„ç‰ˆæœ¬å…¼å®¹é—®é¢˜æ¯”è¾ƒå®¹æ˜“å‡ºé”™ é¦–å…ˆæˆ‘ä»¬æ£€æŸ¥torchå’Œcudaçš„ç‰ˆæœ¬ï¼š

```python
import torch
print(torch.__version__)
```
è¿™é‡Œéœ€è¦æ³¨æ„å¦‚æœæƒ³è¦ä½¿ç”¨pipè¿›è¡Œå®‰è£…ï¼Œå¿…é¡»æ»¡è¶³cuda>=12.1ï¼š
```shell
pip install autoawq
```
å¯¹äº CUDA 11.8ã€ROCm 5.6 å’Œ ROCm 5.7ï¼Œæ¨èä»æºç è¿›è¡Œå®‰è£…ï¼š

```shell
git clone https://github.com/casper-hansen/AutoAWQ.git
cd AutoAWQ
pip install -e .
```
#### åŠ è½½æ¨¡å‹
AWQå®Œå…¨å…¼å®¹transformersï¼Œä½ å¯ä»¥ç›´æ¥ç²˜è´´huggingfaceçš„æ¨¡å‹è·¯å¾„ã€‚

æˆ–è€…ä¹Ÿå¯ä»¥ç›´æ¥å°†æ¨¡å‹è·¯å¾„æ›¿æ¢ä¸ºæœ¬åœ°åŠ è½½ä¸‹è½½å¥½çš„æ¨¡å‹æˆ–è€…æ˜¯ä½ å·²ç»å¾®è°ƒå¥½çš„æ¨¡å‹ï¼š
```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer
# model_pathæ˜¯æ¨¡å‹çš„è·¯å¾„ï¼Œè¿™é‡Œä»huggingfaceåŠ è½½Yiæ¨¡å‹ï¼Œå¦‚æœä½ æœ‰å·²ç»å¾®è°ƒå¥½çš„Yiæ¨¡å‹ï¼Œä½ åŒæ ·ç›´æ¥æ›¿æ¢model_pathå³å¯
model_path = '01-ai/Yi-1.5-6B-Chat'
# quant_pathä¸ºé‡åŒ–åæ¨¡å‹çš„è·¯å¾„
quant_path = 'Yi-1.5-6B-Chat-awq'
quant_config = { "zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM" }

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoAWQForCausalLM.from_pretrained(
    model_path
)
tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    trust_remote_code=True
)
```
#### ä¿å­˜æ¨¡å‹
é‡åŒ–åçš„æ¨¡å‹å¯ä»¥ç›´æ¥ä¿å­˜ï¼š
```python
# ä¿å­˜æ¨¡å‹
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)

print(f'Model is quantized and saved at "{quant_path}"')
```
åŒæ ·ä¹Ÿå¯ä»¥ç›´æ¥å°†æ¨¡å‹æŒ‚è½½åˆ°äº‘ç›˜ä¸Šï¼Œæ›´å¿«é€Ÿæ–¹ä¾¿çš„ä¸‹è½½ï¼š
```python
from google.colab import drive
import shutil

# æŒ‚è½½Googleäº‘ç«¯ç¡¬ç›˜
drive.mount('/content/drive')

# å°†æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹åŒæ­¥åˆ°äº‘ç«¯ç¡¬ç›˜çš„æŒ‡å®šè·¯å¾„
# å‡è®¾ä½ æƒ³è¦åŒæ­¥çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ä½äºColabçš„å½“å‰å·¥ä½œç›®å½•ä¸‹
# å¹¶ä¸”ä½ æƒ³è¦å°†å…¶åŒæ­¥åˆ°äº‘ç«¯ç¡¬ç›˜çš„MyDrive/Yi-1.5-6B-Chat-awqæ–‡ä»¶å¤¹ä¸­

# å®šä¹‰æœ¬åœ°æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„è·¯å¾„
local_path = 'Yi-1.5-6B-Chat-awq'

# å®šä¹‰äº‘ç«¯ç¡¬ç›˜çš„ç›®æ ‡è·¯å¾„
drive_path = '/content/drive/MyDrive/Yi-1.5-6B-Chat-awq'

# åŒæ­¥æ“ä½œ
# ä½¿ç”¨copytree
shutil.copytree(local_path, drive_path)
print(f"æ–‡ä»¶å¤¹'{local_path}'å·²åŒæ­¥åˆ°'{drive_path}'ã€‚")

```
#### ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹
æˆ‘ä»¬é€šè¿‡transformersç›´æ¥å¯ä»¥ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
# model_path = quant_path
model_path = 'Yi-1.5-6B-Chat-awq'

tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype='auto'
).eval()
# æç¤ºè¯
messages = [
    {"role": "user", "content": "hi"}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

print(response)
```