### ğŸŒŸä½¿ç”¨AutoGPTQé‡åŒ–

AutoGPTQæ˜¯ä¸€ä¸ªåŸºäº GPTQ ç®—æ³•ï¼Œç®€å•æ˜“ç”¨ä¸”æ‹¥æœ‰ç”¨æˆ·å‹å¥½å‹æ¥å£çš„å¤§è¯­è¨€æ¨¡å‹é‡åŒ–å·¥å…·åŒ…ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡AutoGPTQé‡åŒ–æˆ‘ä»¬çš„Yiç³»åˆ—æ¨¡å‹ã€‚

| æ¨¡å‹ | æ˜¾å­˜ä½¿ç”¨ | ç¡¬ç›˜å ç”¨ |
|--|------|------|
| Yi-1.5-6B-Chat | 7G   | 27G  |

#### å®‰è£…
æ¨èä»æºç è¿›è¡Œå®‰è£…ï¼š

```shell
git clone https://github.com/AutoGPTQ/AutoGPTQ
cd AutoGPTQ
pip install .
```
#### åŠ è½½æ¨¡å‹

åŒæ ·çš„æˆ‘ä»¬è¿˜æ˜¯å¯ä»¥é€šè¿‡transformersåŠ è½½æ¨¡å‹ï¼Œæˆ–è®¸åŠ è½½å¾®è°ƒå¥½çš„æ¨¡å‹ã€‚

åªéœ€è¦æ›¿æ¢æ¨¡å‹è·¯å¾„å³å¯ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ã€‚

```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from transformers import AutoTokenizer

# é…ç½®é‡åŒ–è¶…å‚æ•°
# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
model_path = "01-ai/Yi-1.5-6B-Chat"
quant_path = "Yi-1.5-6B-Chat-GPTQ"
quantize_config = BaseQuantizeConfig(
    bits=8, # é‡åŒ–ä¸º8-bit æ¨¡å‹
    group_size=128, # æ¨è128
    damp_percent=0.01,
    desc_act=False,  # è®¾ä¸º False å¯ä»¥æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦
)

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)
```
#### é‡åŒ–ä¸ä¿å­˜æ¨¡å‹
model.quantize(examples)ä¸­æ ·æœ¬çš„æ•°æ®ç±»å‹åº”è¯¥ä¸º List[Dict]ï¼Œå…¶ä¸­å­—å…¸çš„é”®æœ‰ä¸”ä»…æœ‰ input_ids å’Œ attention_maskã€‚

è¿™é‡Œéœ€è¦æ³¨æ„è‡ªå·±çš„æ•°æ®æ ¼å¼!
```python
import torch
examples = []
messages = [
    {"role": "user", "content": "hi"},
    {"role": "assistant", "content": "Hello! It's great to see you today. How can I assist you"}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
model_inputs = tokenizer([text])
input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)
examples.append(dict(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id)))
model.quantize(examples)

model.save_quantized(quant_path, use_safetensors=True)
tokenizer.save_pretrained(quant_path)
```
#### ä½¿ç”¨æ¨¡å‹
```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

from transformers import AutoTokenizer, GenerationConfig

quantized_model_dir = 'Yi-1.5-6B-Chat-GPTQ'

tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,
                                          use_fast=True,
                                          trust_remote_code=True)

model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,  
                                           device_map="auto", 
                                           use_safetensors=True, 
                                           trust_remote_code=True)


output = tokenizer.decode(model.generate(
    **tokenizer("<|im_start|>user Hi!<|im_end|> <|im_start|>assistant", return_tensors="pt").to(model.device),
    max_new_tokens=512)[0]
                          )
print(output)
```