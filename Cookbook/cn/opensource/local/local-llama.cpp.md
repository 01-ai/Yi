### ğŸŒŸä½¿ç”¨llama.cppæœ¬åœ°è¿è¡Œ

llama.cppæ˜¯åœ¨å„ç§ç¡¬ä»¶ï¼ˆæœ¬åœ°å’Œäº‘ç«¯ï¼‰ä¸Šä»¥æœ€å°‘çš„è®¾ç½®å’Œæœ€å…ˆè¿›çš„æ€§èƒ½å®ç° LLM æ¨ç†ã€‚llama.cppæ˜¯ç”±C++ç¼–å†™è€Œæˆã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨llama.cppä¸­ä½¿ç”¨Yiç³»åˆ—çš„GGUFæ ¼å¼çš„æ¨¡å‹ã€‚

ä»¥ä¸‹æ•™å­¦æˆ‘ä»¬ä½¿ç”¨ä»[huggingface](https://huggingface.co/models?search=yi-1.5-GGUF)ä¸­ä¸‹è½½Yi-1.5-6B-Chat-GGUFæ¨¡å‹æ¥è¿›è¡Œæµ‹è¯•æ•™å­¦ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥é€‰æ‹©Yiç³»åˆ—çš„å…¶å®ƒæ¨¡å‹ï¼Œä½†æ˜¯å¿…é¡»æ³¨æ„âš ï¸çš„æ˜¯æ¨¡å‹æ–‡ä»¶å¿…é¡»æ˜¯GGUFæ ¼å¼ã€‚

#### å‡†å¤‡GGUFæ ¼å¼çš„æ¨¡å‹

##### 1.å®‰è£… huggingface_hub(å¦‚æœä½ æœ‰æœ¬åœ°å·²ç»å¾®è°ƒæˆ–è€…é‡åŒ–å¥½çš„æ¨¡å‹ä½ å¯ä»¥ä¸ç”¨æ‰§è¡Œ)

ç›®çš„æ˜¯ä»huggingfaceä¸‹è½½æ¨¡å‹ï¼š

``````bash
pip install huggingface_hub
``````

å¯ä»¥ä»huggingfaceä¸­ç›´æ¥ä¸‹è½½GGUFæ ¼å¼çš„Yiæ¨¡å‹[lmstudio-community/Yi-1.5-6B-Chat-GGUF](https://huggingface.co/lmstudio-community/Yi-1.5-6B-Chat-GGUF)ã€‚

å¦‚æœä½ æƒ³ç›´æ¥ä¸‹è½½GGUFæ ¼å¼çš„æ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤å¼€å§‹ä¸‹è½½æ¨¡å‹(ä¸å»ºè®®ï¼Œå› ä¸ºLM studioæä¾›çš„æ˜¯Yiæ¨¡å‹çš„GGUFæ ¼å¼åŒ…ï¼Œä¼šå æ®å¾ˆå¤šç¡¬ç›˜ï¼Œç¡¬ç›˜å¤§çš„å¯ä»¥è€ƒè™‘)ï¼š

``````bash
huggingface-cli download lmstudio-community/Yi-1.5-6B-Chat-GGUF --local-dir /root/yi-models/Yi-1.5-6B-Chat-GGUF
``````

##### 2.è½¬æ¢æˆGGUFæ ¼å¼è¿›è¡Œä½¿ç”¨

æˆ‘ä»¬è¿˜æ˜¯å…ˆä»huggingfaceä¸‹è½½Yi-1.5-6B-Chatæ¨¡å‹ï¼Œç„¶åå†é€šè¿‡llama.cppè½¬æ¢ä¸ºGGUFæ ¼å¼ã€‚

``````bash
huggingface-cli download 01-ai/Yi-1.5-6B-Chat --local-dir /root/yi-models/Yi-1.5-6B-Chat
``````

è½¬æ¢ä¸ºGGUFæ ¼å¼ï¼Œä½†æ˜¯ä½ è¦ä½“æ£€ä¸‹è½½å®‰è£…llama.cppä½ å¯ä»¥å‚è€ƒä¸‹ä¸€èŠ‚çš„[ä¸‹è½½å®‰è£…](#ä¸‹è½½å®‰è£…)ã€‚

âš ï¸æ³¨æ„convert-hf-to-gguf.pyåœ¨llama.cppä¸‹ï¼Œæ‰€ä»¥ç›´æ¥åœ¨llama.cpp baseè·¯å¾„æ‰§è¡Œå³å¯ï¼š

``````
python convert-hf-to-gguf.py /root/yi-models/Yi-1.5-6B-Chat --outfile /root/yi-models/Yi-1.5-6B-Chat-GGUF/Yi-1.5-6B-Chat-q8_0-v1.gguf --outtype q8_0
``````

è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªGGUFæ ¼å¼çš„æ¨¡å‹ç›®å½•åœ¨ï¼ˆ/root/yi-models/Yi-1.5-6B-Chat-GGUFï¼‰ã€‚

#### ä¸‹è½½å®‰è£…

##### 2.ä»æºç ä¸‹è½½llama.cpp

``````bash
git clone https://github.com/ggerganov/llama.cpp
``````

``````bash
cd llama.cpp
``````

##### 3.ç¼–è¯‘

æˆ‘ä»¬æä¾›ä¸¤ä¸ªç‰ˆæœ¬æ¥è¿›è¡Œç¼–è¯‘ï¼Œè¯·æŸ¥çœ‹ä½ çš„ç”µè„‘æ˜¯cpuè¿˜æ˜¯gpuç‰ˆæœ¬

æŸ¥çœ‹ç‰ˆæœ¬ï¼š

``````python
import torch # å¦‚æœpytorchå®‰è£…æˆåŠŸå³å¯å¯¼å…¥
print(torch.cuda.is_available()) # æŸ¥çœ‹CUDAæ˜¯å¦å¯ç”¨ï¼Œå¦‚æœTrueè¡¨ç¤ºå¯ä»¥ä½¿ç”¨ï¼Œåä¹‹åˆ™ä½¿ç”¨cpuç‰ˆæœ¬
``````

ä½¿ç”¨cmake ç”Ÿæˆ Makefile(cudaç‰ˆæœ¬)ï¼š

``````bash
cmake -B build_cuda -DLLAMA_CUDA=ON
cmake --build build_cuda --config Release -j 8
``````

ä½¿ç”¨cmake ç”Ÿæˆ Makefile(cpuç‰ˆæœ¬)ï¼š

``````bash
cmake -B build_cpu
cmake --build build_cpu --config Release
``````

##### 4.å¼€å§‹è¿è¡Œ

é¦–å…ˆåˆ‡æ¢ç›®å½•åˆ°binç›®å½•ä¸‹è¿›è¡Œæ‰§è¡Œï¼š

``````bash
cd build_cuda or cd build_cpu
``````

``````bash
cd bin
``````

é€šè¿‡mainè¿›è¡Œæ‰§è¡Œï¼Œæ‰§è¡Œå‘½ä»¤æœ‰è®¸å¤šå¯ä»¥è°ƒèŠ‚çš„å‚æ•°ï¼Œä½ å¯ä»¥å‚è€ƒ[è¿™é‡Œ](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)è¿›è¡Œä¿®æ”¹ã€‚

é€‚ç”¨äºLinuxã€macOS ã€‚

``````bash
./llama-cli -m /root/yi-models/Yi-1.5-6B-Chat-GGUF/Yi-1.5-6B-Chat-q8_0-v1.gguf -n -1 --color -r "User:" --in-prefix " " -i -p \
'User: ä½ å¥½
AI: ä½ å¥½æˆ‘æ¥è‡ªé›¶ä¸€ä¸‡ç‰©ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨ï¼Ÿ
User: å¥½å•Š
AI: ä½ æƒ³èŠèŠä»€ä¹ˆè¯é¢˜å‘¢ï¼Ÿ
User:'
``````

é€‚ç”¨äºWindowsï¼š

``````bash
llama-cli.exe -m /root/yi-models/Yi-1.5-6B-Chat-GGUF/Yi-1.5-6B-Chat-q8_0-v1.gguf -n -1 --color -r "User:" --in-prefix " " -i -e -p "User: ä½ å¥½\nAI: ä½ å¥½æˆ‘æ¥è‡ªé›¶ä¸€ä¸‡ç‰©ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨ï¼Ÿ\nUser: å¥½å•Š!\nAI: ä½ æƒ³èŠèŠä»€ä¹ˆè¯é¢˜å‘¢ï¼Ÿ\nUser:"
``````

è¿è¡Œåå³å¯è¿›è¡Œå¯¹è¯äº†
![llama.cpp](../../assets/llama-cpp-0.jpg)



##### 5.ä½¿ç”¨llama.cppé‡åŒ–

å¦‚æœä½ æƒ³ä½¿ç”¨llama.cppè¿›è¡Œé‡åŒ–ï¼Œä½ å¯ä»¥æ‰§è¡Œ

``````
./llama-quantize --allow-requantize /root/yi-models/Yi-1.5-6B-Chat-GGUF/Yi-1.5-6B-Chat-q8_0-v1.gguf /root/yi-models/Yi-1.5-6B-Chat-GGUF/Yi-1.5-6B-Chat-q4_1-v1.gguf Q4_1
``````

æ‰§è¡Œå®Œæ¯•åä½ å°±å¾—åˆ°äº†é‡åŒ–åçš„Yi-1.5-6B-Chat-q4_1-v1.ggufåœ¨</root/yi-models/Yi-1.5-6B-Chat-GGUF/Yi-1.5-6B-Chat-q4_1-v1.gguf>ç›®å½•ä¸‹ã€‚

ä½¿ç”¨å’Œå‰é¢ä¸€æ ·ï¼Œå¯ä»¥[å‚è€ƒ](#4å¼€å§‹è¿è¡Œ)

è¿™åªæ˜¯ä¸€ä¸ªå®ä¾‹é‡åŒ–ä¸ºQ4_1ï¼Œä½ å¯ä»¥æ‰§è¡Œå¦‚ä¸‹æŒ‡ä»¤æŸ¥çœ‹å…¶å®ƒç”¨æ³•ï¼š

``````
./llama-quantize -h
``````

éµå®ˆllama.cppçš„ç”¨æ³•ï¼Œé‡åŒ–æˆå…¶å®ƒç²¾åº¦å‡å¯å…·ä½“å‚è€ƒå¦‚ä¸‹ï¼š

``````
type for the output.weight tensor
  --token-embedding-type ggml_type: use this ggml_type for the token embeddings tensor
  --keep-split: will generate quatized model in the same shards as input  --override-kv KEY=TYPE:VALUE
      Advanced option to override model metadata by key in the quantized model. May be specified multiple times.
Note: --include-weights and --exclude-weights cannot be used together

Allowed quantization types:
   2  or  Q4_0    :  3.56G, +0.2166 ppl @ LLaMA-v1-7B
   3  or  Q4_1    :  3.90G, +0.1585 ppl @ LLaMA-v1-7B
   8  or  Q5_0    :  4.33G, +0.0683 ppl @ LLaMA-v1-7B
   9  or  Q5_1    :  4.70G, +0.0349 ppl @ LLaMA-v1-7B
  19  or  IQ2_XXS :  2.06 bpw quantization
  20  or  IQ2_XS  :  2.31 bpw quantization
  28  or  IQ2_S   :  2.5  bpw quantization
  29  or  IQ2_M   :  2.7  bpw quantization
  24  or  IQ1_S   :  1.56 bpw quantization
  31  or  IQ1_M   :  1.75 bpw quantization
  10  or  Q2_K    :  2.63G, +0.6717 ppl @ LLaMA-v1-7B
  21  or  Q2_K_S  :  2.16G, +9.0634 ppl @ LLaMA-v1-7B
  23  or  IQ3_XXS :  3.06 bpw quantization
  26  or  IQ3_S   :  3.44 bpw quantization
  27  or  IQ3_M   :  3.66 bpw quantization mix
  12  or  Q3_K    : alias for Q3_K_M
  22  or  IQ3_XS  :  3.3 bpw quantization
  11  or  Q3_K_S  :  2.75G, +0.5551 ppl @ LLaMA-v1-7B
  12  or  Q3_K_M  :  3.07G, +0.2496 ppl @ LLaMA-v1-7B
  13  or  Q3_K_L  :  3.35G, +0.1764 ppl @ LLaMA-v1-7B
  25  or  IQ4_NL  :  4.50 bpw non-linear quantization
  30  or  IQ4_XS  :  4.25 bpw non-linear quantization
  15  or  Q4_K    : alias for Q4_K_M
  14  or  Q4_K_S  :  3.59G, +0.0992 ppl @ LLaMA-v1-7B
  15  or  Q4_K_M  :  3.80G, +0.0532 ppl @ LLaMA-v1-7B
  17  or  Q5_K    : alias for Q5_K_M
  16  or  Q5_K_S  :  4.33G, +0.0400 ppl @ LLaMA-v1-7B
  17  or  Q5_K_M  :  4.45G, +0.0122 ppl @ LLaMA-v1-7B
  18  or  Q6_K    :  5.15G, +0.0008 ppl @ LLaMA-v1-7B
   7  or  Q8_0    :  6.70G, +0.0004 ppl @ LLaMA-v1-7B
   1  or  F16     : 14.00G, -0.0020 ppl @ Mistral-7B
  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B
   0  or  F32     : 26.00G              @ 7B
          COPY    : only copy tensors, no quantizing
``````
