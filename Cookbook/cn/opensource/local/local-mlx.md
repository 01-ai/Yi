### ğŸŒŸä½¿ç”¨MLX-LMæœ¬åœ°è¿è¡Œ

MLX-LMæ˜¯ä¸€æ¬¾é€‚ç”¨Mac osè¿›è¡Œæœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹çš„æ¡†æ¶ï¼Œå…·ä½“å†…å®¹å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://github.com/ml-explore/mlx-examples/tree/main?tab=readme-ov-file)ã€‚

âš ï¸è¯·æ³¨æ„MLX-LMä»…é€‚ç”¨äºMac osæ“ä½œç³»ç»Ÿã€‚

#### ä¸‹è½½å’Œå®‰è£…

``````bash
pip install mlx-lm
``````

#### å¼€å§‹ä½¿ç”¨

ä»¥ä¸‹ä½¿ç”¨mlx-community/Yi-1.5-6B-Chat-8bitä½œä¸ºç¤ºä¾‹ã€‚

åŒæ ·çš„ä¹Ÿå¯ä»¥æ›¿æ¢ä¸ºå…¶å®ƒæ¨¡å‹ï¼Œä¾‹å¦‚ mlx-community/Yi-1.5-34B-Chat-4bitã€‚

``````python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Yi-1.5-6B-Chat-8bit")

response = generate(model, tokenizer, prompt="hello", verbose=True)
``````